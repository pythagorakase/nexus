how many parameters is it still or do we not care i think we should just Okay so usually when we do these it's to talk
0:06
about a new product that we're about to launch um but we're gonna do something a little bit different today which is to talk about the research that went into
0:11
our product when we launched GPT4.5 we thought people were going to like it we were very proud of the model but people liked it much more than we thought
0:17
people would said all kinds of things like "I never thought I was going to have this experience talking about model it's so different than GPT4 it's way
0:23
better in these ways that are either obvious or hard to explain or this or that." But uh there was like a lot of
0:28
interest about what went into making GPT4.5 so today we have some of the key
0:33
team that made GPT4.5 and we're going to talk about it uh we're going to talk about sort of like what went into it
0:38
what we learned and just like what it takes to make a giant model like this um actually maybe we start with that what
0:44
does it take to make a giant model like this you go first
0:50
uh a lot of people and a lot of time and a lot of compute oh maybe you guys should introduce yourselves alex you
0:55
want to start sure uh yeah hi I'm Alex uh I work a lot on pre-tuning data i also led a pre-tuning ML for GP4.5
1:03
uh I'm Amin Chian i I'm open chief system architect i oversee systems and
1:09
networking broadly at OpenAI i'm Dan i work on data efficiency and algorithms
1:16
yeah okay so what what goes into it um yeah so I think we started this project
1:21
basically two years ago or or so um and uh we kind of knew that we had a big new
1:28
cluster coming online uh and we we kind of saw this on the horizon and we started doing a bunch of work to kind of
1:34
convince ourselves uh of the features that we wanted to include in the run doing a lot of large derisking runs
1:40
building out a very long plan for this um and kind of across the full stack from systems ML everything um and yeah
1:48
it was it was a long story of execution for uh d-risking it uh and kind of preparing for the run um before the run
1:54
itself which you know itself was a very large endeavor yeah I think it's a process that starts
2:01
at inception with a collaboration between ML side and the system side and goes all the way to the time that we
2:08
know what model precisely we want to train and then it's starting the
2:14
uh run process in itself with the pace that we are working at and especially
2:20
trying to make use of our most recent computer that is made available to us it
2:25
becomes something that is difficult to priority plan perfectly mhm so we almost
2:31
always go into a launch with a lot of unresolved issues
2:36
um and try to make forward progress throughout the run despite all the challenges basically add more compute
2:42
resolve all the issues that we probably might not have anticipated despite all
2:47
the projections that we had both on the ML side and the system side and try to
2:52
basically close the gap between what we predicted should happen and what is happening uh I think that is at a very
3:01
high level uh and broadest stroke is the entirety of the process and the tail end
3:07
of it is the execution which takes a lot of people a lot of energy and momentum
3:13
for a prolonged period of time to go through the training process how how close do you feel like we were
3:19
to what we expected to happen to what actually happened um on the usually at
3:26
the beginning uh I'm talking about the system side of it u we are usually far
3:32
away from where we expect it to be and there is always a choice to delay the
3:37
launch and basically defer until more and more issues are resolved or launch
3:42
early and try to basically figure it out as we go it's always a balance of figuring out not to delay the process
3:51
unreasonably uh but almost always there are issues that we don't necessarily know at the
3:58
inception that we are going to run into and the entirety of the process is try
4:03
to uh handle the known to the extent that we can and have a plan for what how
4:10
the run should go and as we make progress just deal with the unknowns which is the
4:18
variability on let's say if a run is successful how long it would take and so
4:23
How far have they been well yeah i think I guess one at the highest level I guess with this project we set out to do
4:29
GPD4.5 which means like 10x smarter than GPD4 so that was sort of I think the the
4:34
initial goal starting like two years ago that we had set our sights on um and then there's a lot that kind of you know
4:41
kind of happened along the way of like oh we think you know can we do better or worse um I think uh it was a very
4:48
complicated row but in the end we got to a model that we feel hit this mark of 10x smarter than 2P4 um in terms of kind
4:55
of the effective compute that we we put into it yeah on this execution side of it of course initially it was far far
5:02
away from how uh long we thought that is it did take longer than we thought uh
5:07
yes but I think the process is to try to shorten it to basically match what we would two two part question about that
5:15
why do why does going from you know making up numbers here 10,000 GPUs to 100,000 GPUs why does that make the
5:20
problem much harder um a lot of issues it's I I do believe that issues that you
5:26
observe at the scale if you have a very keen eye you would observe them at a
5:31
smaller scale it's not that they only manifest at larger scale but something that is a rare occurrence becomes
5:37
something that is catastrophic at a scale uh especially if you haven't anticipated it being what are some of
5:44
the kinds of things that have become captured I mean among those things is that I think is quite well known is uh
5:52
issues with uh the infrastructure uh the failure rates that you observe the the
5:57
variety of failures that you observe uh in both in terms of the types of
6:03
failures and also the the count itself so we get to observe something that I'm
6:10
sure the vendor hasn't observed because this is a large pool of samples and we
6:15
get to observe the entirety of the statistical distribution of uh a large
6:21
pool of resources that that we are executing on the fabric the network fabric is always part of it the
6:27
individual accelerators part of it but at the end of the day this is the beauty of it at at the same time that almost
6:35
everything needs to work as expected for the result to hold and the job is to basically minimize that variance second
6:42
part of the question obviously it's really hard this is for all of you really hard to do things at the edge of scale um so you know even going as we go
6:49
off and do the next training run even kind of crazier um but I've also noticed it gets much easier to go do things that
6:56
are now no longer frontier so it took like hundreds of people almost all of OpenAI's effort to do GPT4.5
7:02
if you guys could go pick whoever you wanted what is the smallest team from OpenAI that could go retrain GPT4 from
7:08
scratch today with everything we know and have and all the systems work i think to get to a GPT4 level model it's
7:15
probably on the order of maybe five to 10 people yeah we did it with that type
7:21
of that number of people with GBT4 4.5 was different in the sense that a lot of
7:28
work history was a lot more people come together and it was a very different effort than before I would say but now
7:34
that we've done that work I think like the stack we've improved a lot and like if you were to retrain like I mean we
7:39
kind of did this a little bit in the pro process of training GB 4.5 we trained GBD40 which was a GP4 caliber model that
7:47
we retrained using a lot of the same stuff coming out of the GP4.5 research program um and I think doing that run
7:53
itself actually took a a smaller number of people right what about from your perspective Dan
8:01
or just sort of like why is why is training big models hard i think doing anything new is hard i think uh even
8:08
just finding out that someone else did something it becomes immensely easier
8:14
because the hard part is having the conviction to do something in the first place i feel like just the fact that
8:20
something is possible is a huge cheat code that just makes it Yeah yeah i mean
8:26
we're always like we're scaling 10x beyond what we did before with these GPT pre-training runs and it's there's
8:31
always new things that you find that are interesting that you couldn't have anticipated necessarily what do we need for the next 10x or 100x in pre-training
8:38
scale data efficiency what does that mean easy answer obviously I know but what does it mean so the the transformer
8:45
the GPT is spectacular at making productive use of of data it it absorbs
8:52
information and it it it compresses and and generalizes to some degree but it's
8:57
its defining character its signature is absorbing the information very efficiently with
9:03
compute but there's a somewhat of a ceiling to how deep of an insight it can
9:10
gain from the data and so at some point as the compute just keeps growing and
9:16
growing and growing and the data grows much less quickly the data becomes the bottleneck of this standard paradigm and
9:24
it requires some algorithmic innovations to be able to learn spend the compute more compute to
9:32
learn more from the same amount of data what do you guys think we need to keep scaling in addition to that i I
9:39
think this answer is system side i think even between different GPTs that we have
9:45
trained GPT 4.5 was the sheer
9:51
uh volume of work that was required to basically the changes that were required
9:57
for us to make uh was a byproduct of the model
10:03
specification the same we wouldn't have been able to train uh GD4.5 on the
10:09
precise same stack as we did GP4 so let's
10:14
say state management our approach with state management changed we had to scale
10:20
to more compute and that compute was not available as part of one cluster we had to go to multicluster training uh and
10:26
imagine it's many many different work streams like that that have to come together in a short period of time for
10:33
us to be able to do this and for making another 10x jump of course and other
10:39
issues that pri we previously knew that they do exist it's just that it's a choice for expediting execution that we
10:47
skip for this one for the next one we have to do it there's no way around it and it's always those choices that
10:52
basically make the timeline of do building the perfect system take much longer uh so we are always compromising
11:00
on what is the fastest way to get to this result the systems is not an end in
11:06
its own the product that the thing that it produces is so for the next 10x for
11:12
me it would be uh of course fall tolerance not uh and a form of fall
11:18
tolerance that we can co-design with the workload such that we don't have to worry the operational burden of keeping
11:26
uh such a massive run going is not like our prior system so I would argue that
11:33
with our prior stack 4.5 but at edge of what we could keep up with do
11:39
you know what percent of steps failed in the 4.5 run due to some component somewhere i actually don't have a number
11:46
off the top of my head but it is usually the way things work this is a
11:52
fascinating thing uh there are issues that are early on uh
12:00
in the lifetime of a new generation of hardware is not necessarily well understood or well studied we
12:09
start the process and we want to make forward progress in presence of such
12:15
issues of course the failure rates are quite significant earlier in the run
12:22
they're not necessarily um it could very well be that once we
12:29
find the root cause and eliminate it the total number of failures significantly
12:35
drops and this is often the case it's just that uh we learn more the infra that is some would call it clean up of
12:41
the infra or understanding uh fundamental issues about the infra uh and the state improves significantly but
12:49
that earlier phase of execution is almost always quite painful because we
12:54
are learning about what are the new failure modes in the new uh uh and the
12:59
new infrastructure while making forward progress of course later on um the
13:05
failure rates drop significantly the uptime improves overall and so on but it's just a matter of prior it is hard
13:12
to predict what this early phase of a generation of infrastructures lifetime
13:18
failure risk would look like and designing for the steady state might result in very poor
13:25
um availability earlier on in the process obviously reasoning models are a
13:31
huge part of our future and you know but but if we were going to put this aside for a second and think about just how far we could go on classical pre-trained
13:38
models assuming we had unlimited GPUs and you know unlimited networking and
13:44
unlimited power but we were stuck with all of our current problems the stuff still broke we didn't figure out fall tolerant training we only had the data
13:50
we have whatever else how and we kind of use the convention of each major number
13:56
of GPT is 100x increment um how far could we go like GPT what could we train
14:02
with what we know today i think on that we get to like a 5.5 ML on the algorithm side I don't I don't
14:07
think there's like a clear limit that we we found i think um yeah I think we're just kind of scratching the surface of
14:13
more data efficient algorithms uh and I think better ways to leverage the data that we have um it's very interesting
14:20
because I think up until this rough point in time like if you look even through GPD4 we were largely just in a
14:26
compute constrained environment um so that was kind of where all the research was going into but now we're you know in
14:32
a very different kind of regime um starting with 4.5 for some aspects of the data where we are much more data
14:38
bound um so there's now not a lot more excitement about this research it is a crazy update that I don't think the
14:44
world has really groed yet i should pick a different one that the world has understood yet uh that we're no longer
14:50
compute constrained on the best models we can produce that's just like that was so the world we lived in for so long
14:55
what was the most interesting ML thing we learned during the four five training
15:01
that you want to share oh
15:07
gosh um or either of you i don't know what I
15:12
could share in general terms I think it is being off of the prediction and
15:18
trying to figure out why we're off of the slope that we predicted to be on yeah I think that most surprising maybe
15:23
we can yeah I think one of the more surprising things that we found um was just uh kind of how different aspects of
15:30
what we were working on on the ML side and what we put into the run scaled and and some things that did or didn't scale
15:36
well um that we we kind of discovered through the process of training this model um it was uh it it's yeah we've
15:44
learned a lot through this i'd say the the two defining characteristics of the GPT paradigm has been that the law you
15:51
can predict the test loss and it scales predictably and magically lower test
15:56
loss means greater intelligence in all these intangible amazing mysterious ways
16:01
are you a maximalist on that do you like fully believe that well I was going to say that one of the
16:07
interesting things we found from 4.5 we tested it again and the model has all of
16:13
these incredibly nuanced abilities that were not in in anyone's bingo card
16:20
specifically we just like the conviction was it's going to be more intelligent in ways that are really hard to
16:26
characterize ahead of time and then you see in the deployment you see in the the
16:31
user satisfaction that it's smarter in all these very subtle ways it has more common sense
16:38
knowledge it understands nuance and context and that's that's the magic that
16:44
came out of the few extra bits of test laws and I think that that part of the
16:49
scaling held up really well what was the most like positive moment of the whole training run what was like what's like
16:54
the favorite memory there's obviously lots of pain but hopefully that's somewhat fit i I do have one but yeah um yeah I can go uh
17:02
yeah so one one that comes to mind for me um we uh kind of we've we worked a
17:08
lot on the ML of the run during the run itself as well and I think some of the the changes that we made during the run
17:14
had a quite quite good impact i think uh maybe perhaps better than anticipated and this was a pretty exciting moment
17:20
for us yeah I I think for me this was probably the
17:26
biggest effort in terms of um I see time
17:32
during the run while we are building I mean I mean running things we are building things in parallel
17:38
um of course to get there faster we are parallelizing work aggressively
17:45
um and there is conviction that it will pay off we will B passes performance
17:52
cliff that makes the model essentially untrainable in terms of the time it would take to to train um and there is a
18:00
plan everybody's executing on it but it's taking long it is a hard it's hard work definitely harder than I imagined
18:06
it would be my projections were off uh by how long it would take to basically
18:12
get uh those issues resolved and seeing I think seeing the moment that the whole
18:18
team once a few of those issues got resolved we got a big performance boost after I
18:25
remember that everybody got I mean you could sense that the energy changed it's
18:31
just that everybody feels excited and now more motivated to push uh everything
18:37
through the end it's just it's fascinating part of see the ETA on our status tracker like yeah has constantly
18:43
shifted from let's say two years to uh something uh tangible uh then the effect
18:52
it has on the morale of the team and everything else is I think that was the
18:57
beauty of it the other part I would uh call out is the ML side of it didn't
19:03
stop the ML code design part of this didn't stop at the launch of uh at the
19:09
launch time and people tagged along issues that were left to say we'll figure it out later people were actively
19:15
working on it and shipping things that helped um with execution time uh and I think
19:22
that spirit of teamwork and basically not having team boundaries of I did my work I pass it on to you is something
19:29
very powerful i was going to say there's been a lot of focus on how the run
19:34
itself was challenging and predictions were but this is despite a tremendous
19:39
amount of sophisticated planning oh this is of course Do you want to say more about it by far the most planning yeah I
19:47
think I mean like I said we we started basically working on this project like a year before we um even started training
19:53
the run itself um and through this we had a number of very large de-risking runs we we took a lot of care to kind of
20:00
very carefully sequence all the changes that we wanted to put into it starting from sort of like very high confidence
20:06
known good config you can think like GPD4 you know we really understand this this setup um from an ML perspective and
20:13
just layering things in and being very careful to very carefully study the the scaling uh of all of the changes we're
20:20
making so it's not just good enough that we see some amount of wind we also want any any win from any new feature to be
20:26
like persistent across scale and not to be tapering off lots of things look good at small scale don't look good at large
20:33
scale uh so we had to be very very paranoid through this process um and really I think we continue to iterate on
20:39
our scaling laws methodology we learned a lot on that front uh through this de-risking process uh that's continuing
20:45
to guide us for future GPGs i I remember something I miss about another very fun
20:50
moment um so there this is the torch sound uh but
20:59
imagine it is very unlikely that we launch a run and it doesn't have bugs i
21:06
mean all sorts of bugs it is just a given that uh but we need to make forward progress and we need to be sure
21:13
that okay are we actually sure that this is on track and these bugs are not super
21:18
negatively affecting the health of the run while we are absolutely sure we were
21:24
initially very sure that there are bugs that are consequential we do we have built a lot of systems around giving us
21:30
visibility and ability to distinguish between is it a hardware fault what type
21:36
of hardware fault it is is it some form of corruption or is it some ML
21:42
potentially an ML bug or something like races in our code um what happened was
21:48
that we we of course had a few open threads about all sorts of issues that
21:54
different symptoms uh all correctness related issues um and at the end of the
22:00
day uh so so of course we have found bugs fixed them and so on we arrived at
22:06
this point that we have multiple open threads and there's a lot of uh thought
22:12
around what is causing all these are they different bugs or it's one bug and causing all of these uh people went I
22:19
mean uh around the table and say everybody vote which one do you think is
22:24
the most probable cause of this bug and the one that turned out to be the bug got the least votes it's just that it is
22:31
a torch some bug a simple summation and uh upstream pietorch and no what was the
22:37
bug so the the bug is for the this is funny because uh for this specific code
22:44
path we were triggering it and for the for context remember that we are mostly
22:49
using triton kernels it's just that for some corner cases that let's say the ops
22:54
don't matter much we basically fall back to I mean it is okay to run torch ops uh
23:00
and our specific uh the specific code path I or data
23:06
triggered for uh in the torch sum function uh was basically had a bug um
23:14
and it was only occurring very infrequently it's just that it's a it's data distribution dependent and it was
23:21
triggering in in a good case illegal memory accesses because it was computing some offsets and some bit of memory the
23:29
fun revelation at the end is that once uh somebody fixed the bug I mean our
23:35
engineers figured out oh I found the bug it is this line is torched some let's ship a fix and see if it fixes
23:41
everything uh it fixed all the way of sanding bugs that where they had
23:46
seemingly distinct uh symptoms and it was quite fun and everybody I think we were renaming the slack channels of
23:53
multiple theory to single bug theory or uh and um that was quite a lot of fun to
24:00
basically okay how when in the when did that happen I can't remember now it was live from very early days of the run up
24:08
until I I think good portion of the run I would um 40% in event do you guys remember how
24:16
someone found it uh I I do remember that i think it was in the list of so imagine
24:22
there there's a sequence of uh kernel invocations and the um the second one
24:29
was the one that was triggering illegal memory accesses and that was some very complicated uh kernel that we wrote um
24:38
and of course our team would suspect that there is a bug there uh obviously there must be a bug there
24:45
and pe I mean several very clever people just line by line everybody is looking
24:52
at it a bug was found but we rolled it out some of those issues disappeared but
24:57
not everything and at some point somebody was I mean in the list torch at
25:04
some was the one feeding inputs to this kernel one of the many inputs of this kernel and somebody started uh one of
25:13
our engineers started looking through the code and different code paths i said oh this very unlikely code path that
25:20
probably most people don't hit we do hit uh and he said okay this line is buggy
25:26
let's I mean of course the only verification and validation we have for it is ship the change observe if all the
25:33
crashes disappear and it did disappear I think that was the validation we needed so but uh it was
25:42
t I mean this is the thing it's just that it's a crash at a very very I mean
25:47
a slow rate it's one every hundred step one every thousand steps and it's something very easy to dismiss
25:54
But it's just that we should not have that in the run as a discipline that we should we do have and it's just not
25:59
giving up on it is the the story
26:05
alex I understand what your life is like i think most people can imagine it like leading up to like pushing go on the run
26:11
but after that happens like what is your day-to-day like are you just like sitting there watching loss curves like how does it go definitely a lot of
26:17
watching lost curves we all did a lot of that um no I think uh past that there's
26:23
a variety of things still trying to work with with systems uh to uh kind of you
26:28
know get out improvements that we didn't get in um on the code design front uh before launch time um there's a lot of
26:35
things that we try to continuously monitor of the run to see if anything is kind of trending like we're not uh
26:41
expecting so this is lost cursor but there's a bunch of other statistics that we can look at um and then yeah kind of
26:47
uh working a bit towards like any other kinds of improvements we could we could do to the run as well from an ML
26:52
perspective u so on the data front it becomes less busy immediately once you click go but uh otherwise there's still
26:58
plenty to be done i I think what we lean on for ML is a lot of correctness imagine there's a lot of noisy signal
27:05
and you are at times reading tea leaves it's just is this healthy or not of course if you wait long enough you would
27:11
know it was it was it healthy or not it's just that the responsibility and how often are there false alarms there
27:16
like how often are you like "Oh this is looking really bad." And then it's fine pretty frequently I think probably about
27:23
half the time maybe because we're a paranoid bunch so I think yeah if it wasn't half the time we wouldn't be
27:28
looking closely enough i have a short lightning round of questions sure if you could have any one ML question answered
27:35
before the next big run what would you most like to know i think uh what we should what what
27:42
algorithms we should employ with uh for for limited data in certain domains is the main thing it's a kind of vague
27:49
question but big answer and if you could have any change to current hardware you
27:54
could have like a new kind of network invented or like a totally new chip architecture what is the most what is
27:59
the limiter on systems at this point not you don't get to like say like oh I want yeah
28:05
so where at this is a transport level
28:11
networking transport level change it's just that where there are faults that
28:16
uh could be worked around uh at a different level than the application level i would rather
28:23
uh the transport the network transport do its job and uh keep running and give
28:30
me the available bandwidth without me worrying about it is there anything promising on that front uh yes we can talk about Okay that's
28:37
good at least um two-part one for you Dan uh how so on the data efficiency
28:44
question humans for whatever other flaws we have about learning things we seem unbelievably data efficient yeah how far away is our very best algorithm
28:52
currently from human level data really hard to measure apples to apples i think
28:57
just like vibes by in language astronomically far away
29:03
100,000 x00x something in that in that range uh it
29:10
depends on whether you count every bit of pixel information on the optical nerve but but we don't know
29:17
algorithmically how to leverage that to be human level at text so I think
29:23
algorithmically we're yeah quite quite quite far away and it apples to apples and then part two is do you think with
29:29
our current our like the direction of our current approach we will get to human level data efficiency or is that just
29:36
not going to happen and doesn't matter well I think for for decades deep learning has been about compute
29:42
efficiency and what's what what's magical besides the data and compute growth is that the the algorithmic
29:50
changes stack so well you've got different people different parts of the world finding this little trick that
29:57
makes it 10% better and then 20% better and they just keep stacking there just hasn't yet been that kind
30:04
of mobilization around data efficiency because it hasn't been worth it because
30:09
when the data is there and your compute limited it's just not worth it and so
30:15
now we're entering a a new stage of AI research where we we'll be stacking data
30:21
efficiency wins 10% here 20% there and I think it would be a little foolish to
30:27
make predictions about it hitting walls that we have no reason
30:33
to predict a wall but but it's there the brain certainly operates on different
30:39
algorithmic principles than anything that's a small tweak around what we're doing so we have to hedge a little bit
30:45
there but I think there's a lot of reason for optimism this next one is the
30:51
same question for all three of you uh yes or no or you can add you can add you can add explanation too will humanity
30:56
ever do a 10 million GPU or greater synchronous pre-training run
31:02
i don't know if it'll exactly be a pre-training run but I think there'll probably be some kind of training run that there will be 10 million GPU
31:07
training yeah yeah yeah i don't know if it'll look it'll probably look totally different than what we're doing today but there will be something that is kind
31:13
of in the spirit of unsupervised learning that is like at that scale i think I think so i would call it
31:19
semi-synchronous uh and the scale of it I hope so i think it sounds very interesting you would call it
31:25
semi-synchronous you said yes i think not fully synchronous but it's just laws of nature can't bend it
31:31
completely i think it's likely to be more
31:37
decentralized than that there'll definitely be 10 million GPUs working together on an AI system that is
31:43
learning and doing things but it might not all the parts of the brain won't necessarily all be talking to each other
31:51
that makes sense um can you say anything that we've learned or observed about how
31:57
uh smarter and bigger pre-trained models correlate with how good a model is at
32:04
learning reasoning um yeah so I think what we've kind of observed is uh it is better
32:11
pre-training and unsupervised learning just tends to lift kind of broad-based intelligence of the model and aid a lot
32:17
in generalization uh which we found to be like really complimentary I think with reasoning
32:22
which can tend to be a little bit spikier lumpier in terms of like where it's increasing intelligence um so yeah
32:29
I think they're they found them to be good compliments to go off on a little bit of a tangent do any of you guys have like a intuition of is it weird or is
32:38
there anything to take away from the fact that pre-training seems to be so general across everything and when we teach a model reasoning we can get it so
32:45
good only at one category of things yeah I don't I don't know if it's the most
32:50
Yeah I think I think it's interesting i think it's kind of not surprising to see this out of pre-training when you're
32:56
just look at what you're what you're training with um you're when you construct like a training data set for pre-training it's inherently very broad
33:02
we're we're targeting breadth and diversity um and I think it's it's kind of difficult to always get the same
33:09
breadth when you talk about doing RL and having like environments that you can kind of cleanly get uh good reward
33:15
signal out of and good good environments out of i I I agree but I think there's another factor that pre-training is
33:23
essentially it's compressing the data and compressing the data is about seeing connections between different things
33:30
it's about analogies it's about uh abstractions
33:35
and reasoning is on a particular problem it there's like there is a skill and a
33:42
craft to thinking carefully and thinking carefully can unlock many different kinds of problem
33:49
solving in different domains but there's something uh more learning at a more
33:54
abstract level when you're compressing across domains in the way pre-training does
34:00
that that makes Oh that I'm going to change my question for you in a second i just thought of something else um what's going to limit us in terms of systems
34:07
progress chips processors memory network or power like what what will be the
34:12
bottleneck of continuing to scale this up um it is this is the beauty of systems
34:19
uh in that if you do code design the workload is adaptable to the infrastructure that you
34:25
built um there is I think let's say there is no statement that broadly network is a
34:31
bottleneck or memory bandwidth is a bottleneck or computer is a bottleneck we have the option to basically shift
34:38
and even for the same specification of the model we have the option of shifting
34:43
resource demands to basically create a more balanced system however having said that I think let's say the pre-training
34:50
answer is different than the inference answer and so on but it it never hurts to have more memory bandwidth uh I would
34:59
uh yes I think this is a hard question to answer and without qualification
35:05
speaking of that earlier point how much do your teams work together on like the spec of the model as we get ready for the four five run like how much how much
35:12
is it you're just like this yeah very closely i mean down to like the shapes of the the map moles that we want to be
35:18
doing um making sure those are optimized well um but for this project it was a much deeper collaboration going back um
35:25
to like I guess six or nine months before the launch of the run um trying to do for some of the the functionality
35:31
we wanted to put into the run and like aspects of what we needed to do to make 4.5 happen we took on like a a specific
35:38
like very large d-risking run uh that was specifically focused on co-design
35:43
with systems uh to make kind of the the ML and the systems work together well at scale um so there we I think it's the
35:50
first time we put like such a large emphasis just on the codeesign aspect was really key yeah I think that was the
35:56
first big um scale effort that I I remember that it is not just about
36:01
fine-tuning one aspect of it is that fundamentally you want a property whole to hold system side and that property
36:10
doesn't emerge out of nowhere you really need to steer the system to give you that property uh so that code design
36:17
effort is something uh that uh formed the architecture and architectural
36:23
elements that goes into the model and somehow ties the system and ML side
36:29
together it's probably a property we would not prefer to have yeah I would ideally want everything to be decoupled
36:35
to basically give maximal room to each other but at times things get tied together where you are really catering
36:42
to the requirements of your infrastructure or how things should be i mean oftentimes it is you really want to
36:49
have a balanced system uh and balanced communication and a very symmetrical
36:55
type of system and you have the best knob we have at our disposal is all code
37:01
design how close are we to the idealized mean system do you mean being like fully
37:07
happy we have all the hardware we want it works for what we know about ML we are nowhere near that you think it is we
37:14
are nowhere near but it is fun it is the the practice of building systems is
37:20
always about that that you have an idealized view of how things should work and it's just about reconciling the
37:25
differences of that with what you have i think we are not doing theory for the
37:30
sake of let's say just talking about what we want it to be we just want to make it happen and approximate that
37:37
ideal to the degree that we can so to be honest I think this
37:44
u is probably as exciting as it gets for systems you be get to really come up
37:51
with hypothesis of what is a good system design uh and take it from there and
37:57
basically see the results in action very quickly things that previously one would say this is an elegant system design and
38:05
uh we have to just history will tell us if this is the right thing to do or the wrong thing to do we have a lot of
38:11
compute we have a problem we we we know the target and just we go and see if our
38:17
choices were correct or not how much is your team thinking about the sort of constraints of system design when
38:24
they're trying to decide what what's going to go into the run yeah I think it's it's a huge consideration for just
38:29
doing pre-training runs at large um I think like since 4.5 a lot of the work on the architecture side there there's
38:35
also kind of ongoing threads around further code design further places that we can design for future hardware uh
38:42
build together i think there's been a lot of promising uh work already since then okay my changed question for you why
38:49
does unsupervised learning work compression so So the ideal intelligence
38:56
is called Solomon induction basically it's it's uncertain about what
39:02
universe it's in and it imagines all possible un it considers all possible
39:08
universes with simple ones more likely than less simple ones and it's it's
39:13
fully basil in head and up updates its views as it progresses and you can approximate
39:21
this by uh finding the the shortest program that computes everything you've
39:28
seen so far and what we're doing with pre-training or one way to think about what pre-training is doing is it is
39:35
compressing it is trying to find the shortest program that explains all of
39:41
the data that humans have produced so far as a way of approximating
39:46
and why does looking at next token prediction do that it's actually a subtle question there there was a
39:52
paradox or somewhat of a paradox in statistics for a long time why do deep
39:59
networks generalize when they don't seem to compress normally in statistics you
40:04
have lots of data and you got small models the models predict the data therefore you can the models must have
40:10
compressed and must have learned something in pre-training generally the models are pretty gigantic and they
40:16
scale roughly with the with the data uh so it was always a question are they
40:22
actually compressing are they generalizing and of course there are critics who have said well it's just
40:27
memorizing and interpolating and superficial um but there's a way of viewing
40:32
pre-training such that you do see that it is a compressor in a in a different unintuitive way and basically the idea
40:40
is called prequential compression and the idea is that the fact that it learns
40:46
quickly during training means you can turn it into a
40:51
great compressor so even though the weights are big the binary doesn't need to store the weights the binary can
40:59
pre-train from scratch to decompress and so the fact that it's learning really
41:05
really quickly means that most of that data you can encode with very very few bits so basically for a subtle reason it
41:12
really is quite a good compressor and I think that is a a pretty satisfying explanation for
41:19
why it really does lead to intelligence you guys have anything to add no it's
41:25
great thank you uh one one thing somewhat relatedly that
41:31
hasn't come up yet is the discipline of metrics it's like the the
41:37
thing the thing you get when you do these scaling laws and you you do all
41:43
the ML science is very dependent on the metric that you choose what do you mean
41:48
what do you want to say oh yeah just talking about the the what test side you're you're evaluating your perplexity
41:54
on so what you're referring to even even looking primarily at perplexity oh yes
41:59
there's already some viewers might think we're looking at college tests or something oh
42:08
yeah um so yeah I mean do you want to explain perplexity then i think it's worth it yeah so it's very it's very
42:14
tempting to try to evaluate your model for intelligence on things that are
42:21
legible to humans as tests uh but if if you if you do this probably you're going
42:28
to be favoring changes that make memorization easier at the cost of
42:33
making systems actually smarter because almost every test that we have out there's something similar online like if
42:41
you can actually train on the entire internet tests become somewhat uh degenerate compared to tests for humans
42:47
where they can't do that and so the the main approach in the field is to look at
42:55
how much it compresses some held out data that's thought to be good data and
43:01
e even then if you're not careful about that held out data it's too similar to your training data training changes to
43:08
your training algorithm that make it memorize better will seem to make it smarter because it'll have already it'll
43:15
already know your test set and we don't want to just be measuring memorization after memorization we're after
43:21
generalization and so out of distribution generalization so yeah this is why I guess yeah maybe you're alluding to so like the the key test
43:27
sets that we're looking at we we care a lot about them not being present in any degree to the slightest degree uh in our
43:33
training set because that kind of throws off all all the way that we do our scaling laws um so yeah this is a very
43:39
key point with what's our best thing for that uh so our our internal codebase uh
43:44
which we know is not out there um yeah it's a very good held out has that held as our best thing across like
43:51
many it's still the best thing yeah it's remarkable i mean we joke that a model
43:56
is its monor repo loss like everything else there's some incredibly meta
44:02
recursive thing there oh yeah
44:09
somehow you you you pre-train the model it has a monor repo loss and somehow
44:15
this tells you so much about how it's going to be behaved down it's it tells
44:20
you a lot about how a philosophy grad student is going to find the nuances of
44:26
its responses but it's incredible it is incredible related to that and sort of last question in some sense this whole
44:33
effort which was hugely expensive in terms of people and time and dollars and everything else was an experiment to
44:41
further validate that the scaling laws keep going and why and turns out they do and they
44:48
probably keep going for a long time um I accept scaling laws like I accept quantum mechanics or something but they
44:54
still don't like I still don't know why like why should that be a property of the universe so why are scaling laws a
45:01
property of the universe you want I can I can take a stab well the the fact that more compression will lead to more
45:07
intelligence that has this very strong philosophical grounding so the question is why does training bigger models for
45:15
longer give you more compression and there are a lot of theories here
45:20
there's the one I like is that the the relevant concepts are sort of uh sparse
45:27
in the in the the data of the world and in particularly it's is a power law so
45:34
that the like the hundth uh most important concept appears in one out of
45:39
a hundred of the documents or or whatever so there's long tales does that mean that if we make a perfect data set
45:44
and figure out very data efficient algorithms i mean can go home it it means that there's potentially
45:50
exponential compute wins on the table to be very s sophisticated about your choice of data but but basically when
45:59
you just scoop up data passively you're going to require 10xing your compute and your
46:07
data to to get the next constant number of things in that tail and there's just that tail keeps
46:14
going it's long you keep you can keep uh mining it although as you alluded to you
46:22
can probably do a lot better i think that's a good place to leave it
46:28
thank you guys very much that was fun yeah thank you