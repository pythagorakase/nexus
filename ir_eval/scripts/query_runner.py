#!/usr/bin/env python3
"""
Query Runner Module for NEXUS IR Evaluation System

This module provides functions for running golden queries against the MEMNON system
and storing results in the IR evaluation database.
"""

import os
import sys
import json
import time
import datetime
import logging
import tempfile
from typing import Dict, List, Any, Tuple, Optional, Set
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from db import IRDatabase

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("nexus.ir_eval.query_runner")

def run_golden_queries(
    settings_path: str,
    golden_queries_path: str,
    config_type: str,
    run_name: str = None,
    queries: Optional[List[str]] = None,
    hybrid: Optional[bool] = None,
    k: int = 10,
    db: Optional[IRDatabase] = None,
    category: Optional[str] = None,
    description: Optional[str] = None,
    chunks_per_query: Optional[int] = None
) -> Optional[int]:
    """
    Run golden queries using the golden_queries_module and save results directly to database.
    
    Args:
        settings_path: Path to settings.json
        golden_queries_path: Path to golden_queries.json
        config_type: Type of configuration ('control' or 'experiment')
        run_name: Name for this run
        queries: Optional list of specific queries to run
        hybrid: Whether to enable hybrid search
        k: Number of results to return for each query
        db: IRDatabase instance
        category: Optional category to filter queries by
        description: Optional description for this run
        chunks_per_query: Maximum number of chunks to retrieve per query
        
    Returns:
        ID of the run in the database
    """
    # Use provided database or create one
    if db is None:
        db = IRDatabase()
    
    # Set default run name if not provided
    if run_name is None:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        run_name = f"{config_type}_{timestamp}"
    
    # Import the golden queries module directly
    try:
        # Add the scripts directory to the Python path
        nexus_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        scripts_dir = os.path.join(nexus_root, "scripts")
        if scripts_dir not in sys.path:
            sys.path.insert(0, scripts_dir)
        
        from golden_queries_module import run_queries
        
        logger.info(f"Imported golden_queries_module - running queries directly")
        
        # Run queries directly through the module
        try:
            limit = len(queries) if queries else None
            
            # Get results directly as Python objects
            results_data = run_queries(
                golden_queries_path=golden_queries_path,
                settings_path=settings_path,
                limit=limit,
                k=k,
                hybrid=hybrid,
                category=category,
                chunks_per_query=chunks_per_query,
            )
            
            logger.info(f"Successfully retrieved results from golden_queries_module")
            
            # Log the first result to verify scores are present
            if results_data and "query_results" in results_data and results_data["query_results"]:
                first_query = results_data["query_results"][0]
                if "results" in first_query and first_query["results"]:
                    first_result = first_query["results"][0]
                    logger.debug(f"First result for query {first_query['query'][:20]}...")
                    
                    if 'vector_score' in first_result:
                        logger.debug(f"  vector_score: {first_result['vector_score']}")
                        logger.debug(f"  vector_score type: {type(first_result['vector_score']).__name__}")
                    
                    if 'text_score' in first_result:
                        logger.debug(f"  text_score: {first_result['text_score']}")
                        logger.debug(f"  text_score type: {type(first_result['text_score']).__name__}")
                    
                    if 'score' in first_result:
                        logger.debug(f"  score: {first_result['score']}")
                        logger.debug(f"  source: {first_result.get('source', 'unknown')}")
            
            # Extract settings from results data
            settings = results_data.get("settings", {})
            
            # Create run in database
            run_description = description
            if not run_description:
                run_description = f"Generated by query_runner.py with {config_type} configuration"
                
            run_id = db.add_run(
                name=run_name, 
                settings=settings, 
                config_type=config_type, 
                description=run_description
            )
            
            if not run_id:
                logger.error("Failed to add run to database")
                return None
            
            # Save query results to database - this is where we'll focus debugging
            query_results = results_data.get("query_results", [])
            
            # Debug: look at text scores before saving to database
            if query_results and query_results[0]["results"]:
                sample_results = query_results[0]["results"][:3]
                logger.debug("Text scores before database insertion:")
                for i, res in enumerate(sample_results):
                    logger.debug(f"  Result {i}: id={res.get('id')}, "
                               f"text_score={res.get('text_score')} "
                               f"({type(res.get('text_score')).__name__})")
            
            if not db.save_results(run_id, query_results):
                logger.error("Failed to save results to database")
                return None
            
            logger.info(f"Saved run {run_id} to database")
            return run_id
            
        except Exception as e:
            logger.error(f"Error in golden_queries_module.run_queries: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    except ImportError as e:
        logger.error(f"Error importing golden_queries_module: {e}")
        logger.error("This indicates the refactoring is incomplete - you need to create scripts/golden_queries_module.py")
        return None

def run_queries_for_specific_ids(
    query_ids: List[int],
    settings_path: str,
    hybrid: bool = False,
    k: int = 10,
    db: Optional[IRDatabase] = None,
    run_name: Optional[str] = None,
    description: Optional[str] = None
) -> Optional[int]:
    """
    Run specific queries by their IDs through MEMNON.
    
    Args:
        query_ids: List of query IDs to run
        settings_path: Path to settings.json
        hybrid: Whether to use hybrid search
        k: Number of results to return
        db: IRDatabase instance
        run_name: Name for this run
        description: Description for this run
        
    Returns:
        ID of the run in the database
    """
    # Use provided database or create one
    if db is None:
        db = IRDatabase()
    
    # Set default run name if not provided
    if run_name is None:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        query_range = f"{min(query_ids)}-{max(query_ids)}"
        run_name = f"queries_{query_range}_{timestamp}"
    
    # Set default description if not provided
    if description is None:
        description = f"Custom run for specific query IDs: {query_ids}"
    
    try:
        # Import MEMNON
        from nexus.agents.memnon.memnon import Memnon
        
        # Load settings
        with open(settings_path, 'r') as f:
            settings_data = json.load(f)
        
        # Extract MEMNON settings
        if "Agent Settings" in settings_data and "MEMNON" in settings_data["Agent Settings"]:
            memnon_settings = settings_data["Agent Settings"]["MEMNON"]
        else:
            logger.error("MEMNON settings not found in settings file")
            return None
        
        # Override hybrid search setting
        memnon_settings["hybrid_search"] = hybrid
        
        # Initialize MEMNON
        memnon = Memnon(settings=memnon_settings)
        
        # Get query texts from database
        cursor = db.conn.cursor()
        
        placeholders = ','.join(['?' for _ in query_ids])
        cursor.execute(f"SELECT id, text, category FROM queries WHERE id IN ({placeholders})", query_ids)
        
        queries = cursor.fetchall()
        if not queries:
            logger.error(f"No queries found with IDs: {query_ids}")
            return None
        
        # Create run in database
        run_id = db.add_run(
            name=run_name,
            settings=memnon_settings,
            config_type="custom",
            description=description
        )
        
        if not run_id:
            logger.error("Failed to add run to database")
            return None
        
        # Run each query and save results
        query_results = []
        
        for query in queries:
            query_id = query["id"]
            query_text = query["text"]
            category = query["category"]
            
            logger.info(f"Running query ID {query_id}: {query_text}")
            
            # Run query through MEMNON
            try:
                # Analyze and run query
                memnon._analyze_query(query_text)
                results = memnon.search(
                    query=query_text,
                    k=k,
                    include_chunks=True
                )
                
                # Format results for database storage
                formatted_results = []
                for i, res in enumerate(results):
                    formatted_results.append({
                        "id": res.get("chunk_id", f"unknown-{i}"),
                        "score": res.get("score", 0.0),
                        "vector_score": res.get("vector_score", 0.0),
                        "text_score": res.get("text_score", 0.0),
                        "text": res.get("raw_text", ""),
                        "source": res.get("source", "")
                    })
                
                # Add results to query results list
                query_results.append({
                    "query_id": query_id,
                    "query": query_text,
                    "category": category,
                    "results": formatted_results
                })
                
                logger.info(f"Got {len(formatted_results)} results for query ID {query_id}")
                
            except Exception as e:
                logger.error(f"Error running query {query_id}: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        # Save all results to database
        if not db.save_results(run_id, query_results):
            logger.error("Failed to save results to database")
            return None
        
        logger.info(f"Saved run {run_id} to database with {len(query_results)} queries")
        return run_id
        
    except Exception as e:
        logger.error(f"Error running queries: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

if __name__ == "__main__":
    # Simple CLI for testing
    parser = argparse.ArgumentParser(description="Run golden queries and save results to database")
    parser.add_argument("--settings", default=None, help="Path to settings.json")
    parser.add_argument("--golden-queries", default=None, help="Path to golden_queries.json")
    parser.add_argument("--config-type", default="test", help="Type of configuration")
    parser.add_argument("--hybrid", action="store_true", help="Use hybrid search")
    parser.add_argument("--k", type=int, default=10, help="Number of results to return")
    parser.add_argument("--category", default=None, help="Category filter")
    parser.add_argument("--query-ids", type=str, default=None, help="Comma-separated list of query IDs to run")
    
    args = parser.parse_args()
    
    # Use default paths if not specified
    settings_path = args.settings
    if not settings_path:
        settings_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "settings.json")
    
    if args.query_ids:
        # Run specific queries by ID
        query_ids = [int(id.strip()) for id in args.query_ids.split(',')]
        run_id = run_queries_for_specific_ids(
            query_ids=query_ids,
            settings_path=settings_path,
            hybrid=args.hybrid,
            k=args.k
        )
        if run_id:
            print(f"Successfully ran queries and saved results with run ID: {run_id}")
        else:
            print("Failed to run queries")
    else:
        # Run golden queries
        golden_queries_path = args.golden_queries
        if not golden_queries_path:
            golden_queries_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "golden_queries.json")
        
        run_id = run_golden_queries(
            settings_path=settings_path,
            golden_queries_path=golden_queries_path,
            config_type=args.config_type,
            hybrid=args.hybrid,
            k=args.k,
            category=args.category
        )
        
        if run_id:
            print(f"Successfully ran golden queries and saved results with run ID: {run_id}")
        else:
            print("Failed to run golden queries")