"""
MEMNON Agent - Unified Memory Access System for Narrative Intelligence

This agent is responsible for all memory operations, including:
- Managing embeddings across multiple models
- Storing and retrieving narrative chunks
- Cross-referencing structured data with vector embeddings
- Implementing specialized query patterns for different memory tiers
- Synthesizing responses using local LLM capabilities
"""

import os
import re
import uuid
import logging
import json
import time
import requests
from typing import Dict, List, Tuple, Optional, Union, Any, Set
from datetime import datetime
from pathlib import Path

import sqlalchemy as sa
from sqlalchemy import create_engine, Column, Table, MetaData, text
from sqlalchemy.dialects.postgresql import UUID, BYTEA, ARRAY
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Session, sessionmaker

# Sentence transformers for embedding
from sentence_transformers import SentenceTransformer

# Letta framework
from letta.agent import Agent
from letta.schemas.agent import AgentState 
from letta.schemas.memory import Memory
from letta.schemas.block import Block, CreateBlock
from letta.schemas.message import Message
from letta.embeddings import EmbeddingEndpoint

# Load settings
def load_settings():
    """Load settings from settings.json file"""
    try:
        settings_path = Path("/Users/pythagor/nexus/settings.json")
        if settings_path.exists():
            with open(settings_path, 'r') as f:
                return json.load(f)
        else:
            print(f"Warning: settings.json not found at {settings_path}")
            return {}
    except Exception as e:
        print(f"Error loading settings: {e}")
        return {}

# Global settings
SETTINGS = load_settings()
MEMNON_SETTINGS = SETTINGS.get("Agent Settings", {}).get("MEMNON", {})
GLOBAL_SETTINGS = SETTINGS.get("Agent Settings", {}).get("global", {})

# Configure logging
log_config = MEMNON_SETTINGS.get("logging", {})
log_level = getattr(logging, log_config.get("level", "INFO"))
log_file = log_config.get("file", "memnon.log")
log_console = log_config.get("console", True)

handlers = []
if log_file:
    handlers.append(logging.FileHandler(log_file))
if log_console:
    handlers.append(logging.StreamHandler())

logging.basicConfig(
    level=log_level,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=handlers
)
logger = logging.getLogger("nexus.memnon")

# LLM settings
MODEL_CONFIG = GLOBAL_SETTINGS.get("model", {})
DEFAULT_MODEL_ID = MODEL_CONFIG.get("default_model", "llama-3.3-70b-instruct@q6_k")

# Database settings
DEFAULT_DB_URL = MEMNON_SETTINGS.get("database", {}).get("url", "postgresql://pythagor@localhost/NEXUS")

# Define SQL Alchemy Base
Base = declarative_base()

# Define ORM models based on the PostgreSQL schema
class NarrativeChunk(Base):
    __tablename__ = 'narrative_chunks'
    
    id = Column(sa.BigInteger, primary_key=True)
    raw_text = Column(sa.Text, nullable=False)
    created_at = Column(sa.DateTime(timezone=True), server_default=sa.func.now())

class ChunkEmbedding(Base):
    __tablename__ = 'chunk_embeddings'
    
    id = Column(sa.BigInteger, primary_key=True)
    chunk_id = Column(sa.BigInteger, sa.ForeignKey('narrative_chunks.id', ondelete='CASCADE'), nullable=False)
    model = Column(sa.String(100), nullable=False)
    embedding = Column(sa.LargeBinary, nullable=False)
    created_at = Column(sa.DateTime(timezone=True), server_default=sa.func.now())
    dimensions = Column(sa.Integer, default=1024)
    
    __table_args__ = (
        sa.UniqueConstraint('chunk_id', 'model', name='uix_chunk_model'),
    )

class ChunkMetadata(Base):
    __tablename__ = 'chunk_metadata'
    
    id = Column(sa.BigInteger, primary_key=True)
    chunk_id = Column(sa.BigInteger, sa.ForeignKey('narrative_chunks.id', ondelete='CASCADE'), nullable=False)
    season = Column(sa.Integer)
    episode = Column(sa.Integer)
    scene = Column(sa.Integer)
    world_layer = Column(sa.String(50))
    time_delta = Column(sa.String(100))
    location = Column(sa.String(255))
    atmosphere = Column(sa.String(255))
    characters = Column(sa.JSON)
    arc_position = Column(sa.String(50))
    direction = Column(sa.JSON)
    magnitude = Column(sa.String(50))
    character_elements = Column(sa.JSON)
    perspective = Column(sa.JSON)
    interactions = Column(sa.JSON)
    dialogue_analysis = Column(sa.JSON)
    emotional_tone = Column(sa.JSON)
    narrative_function = Column(sa.JSON)
    narrative_techniques = Column(sa.JSON)
    thematic_elements = Column(sa.JSON)
    causality = Column(sa.JSON)
    continuity_markers = Column(sa.JSON)
    metadata_version = Column(sa.String(20))
    generation_date = Column(sa.DateTime)

class Character(Base):
    __tablename__ = 'characters'
    
    id = Column(sa.BigInteger, primary_key=True)
    name = Column(sa.String(50), nullable=False)
    aliases = Column(ARRAY(sa.String))
    summary = Column(sa.String(500), nullable=False)
    appearance = Column(sa.Text, nullable=False)
    background = Column(sa.Text, nullable=False)
    personality = Column(sa.Text, nullable=False)
    conflicts = Column(sa.Text)
    emotional_state = Column(sa.String(500), nullable=False)
    undisclosed_internal = Column(sa.Text)
    current_activity = Column(sa.String(500))
    current_location = Column(sa.String(500))
    extra_data = Column(sa.JSON)
    created_at = Column(sa.DateTime(timezone=True), server_default=sa.func.now())
    updated_at = Column(sa.DateTime(timezone=True), server_default=sa.func.now(), onupdate=sa.func.now())

class Place(Base):
    __tablename__ = 'places'
    
    id = Column(sa.BigInteger, primary_key=True)
    name = Column(sa.String(50), nullable=False)
    type = Column(sa.String, nullable=False)
    location = Column(sa.String(250), nullable=False)
    summary = Column(sa.String(1000), nullable=False)
    inhabitants = Column(ARRAY(sa.String))
    historical_significance = Column(sa.Text)
    current_status = Column(sa.String(500))
    undiscovered = Column(sa.Text)
    extra_data = Column(sa.JSON)
    created_at = Column(sa.DateTime(timezone=True), server_default=sa.func.now())
    updated_at = Column(sa.DateTime(timezone=True), server_default=sa.func.now(), onupdate=sa.func.now())


class MEMNON(Agent):
    """
    MEMNON (Unified Memory Access System) agent responsible for accessing and 
    retrieving narrative information across all memory types.
    """
    
    def __init__(self, 
                 interface, 
                 agent_state: Optional[AgentState] = None,
                 user = None,
                 db_url: str = None,
                 model_id: str = None,
                 model_path: str = None,
                 debug: bool = None,
                 **kwargs):
        """
        Initialize MEMNON agent with unified memory access capabilities.
        
        Args:
            interface: Interface for agent communication
            agent_state: Agent state from Letta framework (optional in direct mode)
            user: User information (optional in direct mode)
            db_url: PostgreSQL database URL
            model_id: LM Studio model ID to use
            model_path: Path to local model file (fallback)
            debug: Enable debug logging
            **kwargs: Additional arguments
        """
        # Handle direct mode (when agent_state is None)
        self.direct_mode = agent_state is None
        
        if self.direct_mode:
            # In direct mode, we skip the parent Agent initialization
            # and just set up the bare minimum we need
            self.interface = interface
            self.agent_state = None  # We don't use this in direct mode
            self.user = user
            self.block_manager = None  # Not used in direct mode
        else:
            # Normal Letta framework mode, initialize parent Agent class
            super().__init__(interface, agent_state, user, **kwargs)
            
            # Initialize specialized memory blocks if not present
            self._initialize_memory_blocks()
        
        # Configure logging level based on settings or parameter
        if debug is None:
            debug = MEMNON_SETTINGS.get("debug", False)
        if debug:
            logger.setLevel(logging.DEBUG)
            logger.debug("Debug logging enabled")
        
        # Store LLM settings
        self.model_id = model_id or DEFAULT_MODEL_ID
        self.model_path = model_path
        logger.info(f"Using LLM model: {self.model_id}")
        
        # Set up database connection
        self.db_url = db_url or DEFAULT_DB_URL
        logger.info(f"Using database URL: {self.db_url}")
        self.db_engine = self._initialize_database_connection()
        self.Session = sessionmaker(bind=self.db_engine)
        
        # Set up embedding models
        self.embedding_models = self._initialize_embedding_models()
        
        # Get model weights from settings
        model_weights = {}
        for model_name, model_config in MEMNON_SETTINGS.get("models", {}).items():
            weight = model_config.get("weight", 0.33)  # Default equal weight
            model_weights[model_name] = weight
        
        # Use default weights if none defined in settings
        if not model_weights:
            model_weights = {
                "bge-large": 0.4,
                "e5-large": 0.4,
                "bge-small-custom": 0.2
            }
            
        # Set debug flag for LLM-directed search
        self.use_llm_search_planning = MEMNON_SETTINGS.get("query", {}).get("use_llm_planning", False)
        
        # Flag to prioritize text search for testing
        self.force_text_first = False
        
        # Get query and retrieval settings from configuration
        query_config = MEMNON_SETTINGS.get("query", {})
        retrieval_config = MEMNON_SETTINGS.get("retrieval", {})
        
        # Configure retrieval settings using values from settings.json
        self.retrieval_settings = {
            "default_top_k": query_config.get("default_limit", 10),
            "max_query_results": retrieval_config.get("max_results", 50),
            "relevance_threshold": query_config.get("min_similarity", 0.7),
            "entity_boost_factor": retrieval_config.get("entity_boost_factor", 1.2),
            "recency_boost_factor": retrieval_config.get("recency_boost_factor", 1.1),
            "db_vector_balance": retrieval_config.get("db_vector_balance", 0.6),  # 60% weight to database, 40% to vector
            "model_weights": model_weights,
            "highlight_matches": query_config.get("highlight_matches", True)
        }
        
        # Log the retrieval settings
        logger.debug(f"Retrieval settings: {json.dumps(self.retrieval_settings, indent=2)}")
        
        # Memory type registry - maps virtual memory tier to actual storage
        self.memory_tiers = {
            "strategic": {"type": "database", "tables": ["events", "threats", "ai_notebook"]},
            "entity": {"type": "database", "tables": ["characters", "places", "factions", "items"]},
            "narrative": {"type": "vector", "collections": ["narrative_chunks"]},
        }
        
        # Query type registry - maps query types to appropriate tables and search methods
        self.query_types = {
            "character": {
                "primary_tier": "entity",
                "primary_tables": ["characters"],
                "secondary_tier": "narrative",
                "secondary_search": "vector_search",
                "extraction_focus": ["character relationships", "character development", "character actions"]
            },
            "location": {
                "primary_tier": "entity",
                "primary_tables": ["places"],
                "secondary_tier": "narrative",
                "secondary_search": "vector_search",
                "extraction_focus": ["location descriptions", "events at locations"]
            },
            "event": {
                "primary_tier": "narrative",
                "primary_tables": ["events"],
                "secondary_tier": "strategic",
                "secondary_search": "vector_search",
                "extraction_focus": ["event details", "event consequences", "event timeline"]
            },
            "theme": {
                "primary_tier": "narrative",
                "primary_search": "vector_search",
                "extraction_focus": ["thematic elements", "symbolic patterns", "motifs"]
            },
            "relationship": {
                "primary_tier": "entity",
                "primary_tables": ["character_relationships"],
                "secondary_tier": "narrative",
                "secondary_search": "vector_search",
                "extraction_focus": ["character interactions", "relationship dynamics", "emotional connections"]
            },
            "narrative": {
                "primary_tier": "narrative",
                "primary_search": "vector_search",
                "extraction_focus": ["plot points", "story progression", "scene setting"]
            }
        }
        
        # Add LLM initialization
        self.llm_initialized = False
        try:
            # Use global settings for LLM parameters
            global_llm_settings = GLOBAL_SETTINGS.get("llm", {})
            
            self.llm_api_base = global_llm_settings.get("api_base", "http://localhost:1234")
            logger.info(f"Initializing LLM connection to {self.llm_api_base}")
            logger.info(f"Using global LLM settings: temperature: {global_llm_settings.get('temperature', 0.8)}, top_p: {global_llm_settings.get('top_p', 0.95)}, top_k: {global_llm_settings.get('top_k', 40)}")
            
            # Simple test request to initialize the model
            test_prompt = MEMNON_SETTINGS.get("prompts", {}).get("initialization_test", "Reply with ONLY the word 'OK' (no explanation or additional text).")
            test_response = self._query_llm(test_prompt, timeout=120)  # Longer timeout for first load
            self.llm_initialized = "OK" in test_response
            
            if self.llm_initialized:
                logger.info("LLM initialized successfully")
            else:
                # Log the entire response for debugging
                logger.warning(f"LLM initialization response didn't contain expected confirmation.")
                logger.warning(f"Raw LLM response: '{test_response}'")
                logger.warning(f"Using prompt: '{test_prompt}'")
        except Exception as e:
            logger.error(f"Error initializing LLM: {e}")
            logger.error(f"Exception type: {type(e).__name__}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            logger.info("Will attempt to initialize LLM on first query")
        
        logger.info("MEMNON agent initialized")
    
    def _initialize_memory_blocks(self):
        """Initialize specialized memory blocks if not present."""
        # Check if memory blocks exist and create if needed
        required_blocks = ["memory_index", "query_templates", "retrieval_stats", "db_schema"]
        
        for block_name in required_blocks:
            if block_name not in self.agent_state.memory.list_block_labels():
                # Create block with default empty content
                block = CreateBlock(
                    label=block_name,
                    value="",
                    limit=50000,  # Generous limit for memory data
                    description=f"Memory {block_name} block"
                )
                # Add block to memory
                self.block_manager.create_block(block=block, agent_id=self.agent_state.id, actor=self.user)
    
    def _initialize_database_connection(self) -> sa.engine.Engine:
        """Initialize connection to PostgreSQL database."""
        try:
            engine = create_engine(self.db_url)
            
            # Verify connection
            connection = engine.connect()
            connection.close()
            
            # Create tables if they don't exist
            Base.metadata.create_all(engine)
            
            logger.info(f"Successfully connected to database at {self.db_url}")
            return engine
        
        except Exception as e:
            logger.error(f"Failed to connect to database: {e}")
            raise ConnectionError(f"Database connection failed: {e}")
    
    def _initialize_embedding_models(self) -> Dict[str, Any]:
        """Initialize embedding models for semantic retrieval."""
        embedding_models = {}
        
        try:
            # Get model configurations from settings
            model_configs = MEMNON_SETTINGS.get("models", {})
            
            # Load each model defined in settings
            for model_name, model_config in model_configs.items():
                logger.info(f"Loading {model_name} embedding model...")
                
                # Get model paths
                local_path = model_config.get("local_path")
                remote_path = model_config.get("remote_path")
                dimensions = model_config.get("dimensions")
                
                # Try loading from local path first
                if local_path and Path(local_path).exists():
                    try:
                        model = SentenceTransformer(local_path)
                        embedding_models[model_name] = model
                        logger.info(f"Loaded {model_name} from local path: {local_path}")
                        continue
                    except Exception as e:
                        logger.warning(f"Failed to load {model_name} from local path: {e}")
                
                # Fall back to remote path if available
                if remote_path:
                    try:
                        model = SentenceTransformer(remote_path)
                        embedding_models[model_name] = model
                        logger.info(f"Loaded {model_name} from remote path: {remote_path}")
                    except Exception as e:
                        logger.warning(f"Failed to load {model_name} from remote path: {e}")
            
            # If no models were loaded from settings, try default paths as fallback
            if not embedding_models:
                logger.warning("No models loaded from settings, trying defaults...")
                
                try:
                    bge_large = SentenceTransformer("BAAI/bge-large-en")
                    embedding_models["bge-large"] = bge_large
                    logger.info("Loaded default BGE-large embedding model")
                except Exception as e:
                    logger.warning(f"Failed to load default BGE-large: {e}")
                
                try:
                    e5_large = SentenceTransformer("intfloat/e5-large-v2")
                    embedding_models["e5-large"] = e5_large
                    logger.info("Loaded default E5-large embedding model")
                except Exception as e:
                    logger.warning(f"Failed to load default E5-large: {e}")
                
                # Try to load fine-tuned BGE-small model from default local path
                default_bge_small_path = Path("/Users/pythagor/nexus/models/bge_small_finetuned_20250320_153654")
                if default_bge_small_path.exists():
                    try:
                        bge_small = SentenceTransformer(str(default_bge_small_path))
                        embedding_models["bge-small-custom"] = bge_small
                        logger.info("Loaded default BGE-small-custom embedding model")
                    except Exception as e:
                        logger.warning(f"Failed to load default BGE-small-custom: {e}")
            
            if not embedding_models:
                logger.error("No embedding models could be loaded. Vector search will not be available.")
            else:
                logger.info(f"Loaded {len(embedding_models)} embedding models: {', '.join(embedding_models.keys())}")
            
            return embedding_models
        
        except Exception as e:
            logger.error(f"Error initializing embedding models: {e}")
            # Return any successfully loaded models rather than failing completely
            return embedding_models

    def generate_embedding(self, text: str, model_key: str = "bge-large") -> List[float]:
        """
        Generate an embedding for the given text using the specified model.
        
        Args:
            text: Text to embed
            model_key: Key of the model to use
            
        Returns:
            Embedding as a list of floats
        """
        if model_key not in self.embedding_models:
            logger.error(f"Model {model_key} not found in available embedding models")
            raise ValueError(f"Model {model_key} not found in available embedding models")
        
        model = self.embedding_models[model_key]
        embedding = model.encode(text)
        
        return embedding.tolist()
    
    def store_narrative_chunk(self, chunk_text: str, metadata: Dict[str, Any]) -> str:
        """
        Store a narrative chunk with embeddings and metadata in PostgreSQL.
        
        Args:
            chunk_text: The text content of the chunk
            metadata: Associated metadata (season, episode, scene_number, etc.)
            
        Returns:
            ID of the stored chunk
        """
        # Create a session
        session = self.Session()
        
        try:
            # Extract or generate chunk ID
            if "chunk_id" in metadata:
                chunk_id_str = metadata["chunk_id"]
                # Remove any existing chunk with this ID
                existing = session.query(NarrativeChunk).filter(NarrativeChunk.id == chunk_id_str).first()
                if existing:
                    logger.info(f"Replacing existing chunk {chunk_id_str}")
                    session.delete(existing)
                    session.commit()
            else:
                # Generate a UUID for the chunk
                chunk_id_str = str(uuid.uuid4())
            
            # Convert to UUID
            chunk_id = uuid.UUID(chunk_id_str) if not isinstance(chunk_id_str, uuid.UUID) else chunk_id_str
            
            # Create narrative chunk
            narrative_chunk = NarrativeChunk(
                id=chunk_id,
                raw_text=chunk_text
            )
            session.add(narrative_chunk)
            session.flush()  # Flush to get the sequence number
            
            # Extract metadata
            season = metadata.get("season")
            episode = metadata.get("episode")
            scene_number = metadata.get("scene_number")
            world_layer = metadata.get("world_layer", "primary")
            
            # Parse characters from chunk content (basic implementation)
            characters_data = self._extract_characters_from_text(chunk_text)
            
            # Create chunk metadata
            chunk_metadata = ChunkMetadata(
                chunk_id=chunk_id,
                world_layer=world_layer,
                season=season,
                episode=episode,
                narrative_vector=json.dumps({
                    "scene_number": scene_number,
                }),
                characters=json.dumps(characters_data),
                setting=json.dumps({"extracted": "auto"}),
                causality=json.dumps({"extracted": "auto"}),
                prose=json.dumps({"style": "default"})
            )
            session.add(chunk_metadata)
            
            # Generate embeddings with all available models
            for model_key in self.embedding_models:
                try:
                    # Generate embedding
                    embedding = self.generate_embedding(chunk_text, model_key)
                    
                    # Convert embedding to bytes for storage
                    try:
                        # First try using pgvector's native array conversion if available
                        try:
                            from pgvector.sqlalchemy import Vector
                            embedding_obj = Vector(embedding)
                            embedding_bytes = embedding_obj.to_bytes()
                        except (ImportError, AttributeError):
                            # Fall back to standard SQLAlchemy array conversion
                            import numpy as np
                            embedding_arr = np.array(embedding, dtype=np.float32)
                            embedding_bytes = embedding_arr.tobytes()
                    except Exception as e:
                        # If all else fails, use basic SQLAlchemy conversion
                        embedding_bytes = sa.dialects.postgresql.ARRAY(sa.Float).bind_processor(None)(embedding)
                    
                    # Create chunk embedding
                    chunk_embedding = ChunkEmbedding(
                        chunk_id=chunk_id,
                        model=model_key,
                        embedding=embedding_bytes
                    )
                    session.add(chunk_embedding)
                    
                except Exception as e:
                    logger.error(f"Error generating embedding with model {model_key}: {e}")
                    logger.error(f"Exception type: {type(e).__name__}")
                    logger.error(f"Exception details: {str(e)}")
            
            # Commit the transaction
            session.commit()
            logger.info(f"Successfully stored chunk {chunk_id} with embeddings")
            
            return str(chunk_id)
        
        except Exception as e:
            session.rollback()
            logger.error(f"Error storing chunk: {e}")
            raise
        
        finally:
            session.close()
    
    def _extract_characters_from_text(self, text: str) -> List[str]:
        """
        Basic implementation to extract character names from text.
        A more sophisticated implementation would use NER or a custom model.
        
        Args:
            text: Chunk text to analyze
            
        Returns:
            List of extracted character names
        """
        # This is a very simplified implementation
        common_names = ["Alex", "Emilia", "Victor", "Zoe", "Max", "Raven"]
        found_names = []
        
        for name in common_names:
            if name in text:
                found_names.append(name)
        
        return found_names
    
    def query_memory(self, 
                   query: str, 
                   query_type: Optional[str] = None, 
                   memory_tiers: Optional[List[str]] = None, 
                   filters: Optional[Dict[str, Any]] = None, 
                   k: int = 10) -> Dict[str, Any]:
        """
        Unified memory query interface for retrieving narrative information.
        Uses LLM-directed search to dynamically select appropriate search strategies.
        
        Args:
            query: The narrative query
            query_type: Optional type of query (character, event, theme, relationship) - LLM will determine if None
            memory_tiers: Optional specific memory tiers to query - LLM plan will be used if None
            filters: Optional filters to apply (time, characters, locations, etc.)
            k: Preferred number of results (will return more for LLM)
            
        Returns:
            Dict containing query results and metadata
        """
        if filters is None:
            filters = {}

        # Process query to understand information need using LLM
        query_info = self._analyze_query(query, query_type)
        search_start_time = time.time()
        
        # Generate search plan using LLM
        search_plan = self._generate_search_plan(query, query_info)
        logger.info(f"Using LLM-generated search plan: {search_plan['explanation']}")
        
        # Determine which memory tiers to access if not specified (can be redundant if plan covers it)
        if not memory_tiers:
            memory_tiers = self._determine_relevant_memory_tiers(query_info)
        
        # Execute each search strategy in priority order
        all_results = {}
        combined_results = []
        search_metadata = {
            "strategies_executed": [],
            "strategy_stats": {},
            "total_results_before_dedup": 0
        }
        
        # Get configured limits from settings
        vector_results_limit = MEMNON_SETTINGS.get("query", {}).get("include_vector_results", 10)
        text_results_limit = MEMNON_SETTINGS.get("query", {}).get("include_text_results", 10)
        llm_results_limit = MEMNON_SETTINGS.get("query", {}).get("llm_results_limit", 20)
        
        # Sort strategies by priority
        strategies = sorted(search_plan["strategies"], key=lambda s: s["priority"])
        
        # Execute each strategy - run ALL strategies defined in the plan
        for strategy in strategies:
            strategy_type = strategy["type"]
            strategy_start = time.time()
            tier_results = [] # Initialize empty list for each strategy
            
            try:
                if strategy_type == "structured_data":
                    # Query structured database tables
                    tables = strategy.get("tables", [])
                    tier_results = self._query_structured_data(query_info, tables, filters, k * 2) # Fetch more initially
                    logger.info(f"Structured data search found {len(tier_results)} results from {len(tables)} tables")
                    
                elif strategy_type == "vector_search":
                    # Query vector store
                    collections = strategy.get("collections", ["narrative_chunks"])
                    # Fetch configured number of vector results
                    tier_results = self._query_vector_search(query, collections, filters, vector_results_limit)
                    logger.info(f"Vector search found {len(tier_results)} results")
                    
                elif strategy_type == "text_search":
                    # Direct text search using SQL
                    keywords = strategy.get("keywords", self._extract_search_keywords(query_info)) # Use helper if not in plan
                    tier_results = self._query_text_search(query_info, keywords, filters, text_results_limit)
                    logger.info(f"Text search found {len(tier_results)} results using {len(keywords)} keywords")
                    
                else:
                    logger.warning(f"Unknown search strategy type: {strategy_type}")
                    continue
                
                # Record strategy results and stats
                strategy_time = time.time() - strategy_start
                search_metadata["strategies_executed"].append(strategy_type)
                search_metadata["strategy_stats"][strategy_type] = {
                    "execution_time": strategy_time,
                    "results_count": len(tier_results)
                }
                
                # Store and extend results
                all_results[strategy_type] = tier_results
                combined_results.extend(tier_results)
                search_metadata["total_results_before_dedup"] += len(tier_results)
                    
            except Exception as e:
                logger.error(f"Error executing search strategy {strategy_type}: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        # Simple deduplication by ID without scoring bias
        seen_ids = set()
        deduplicated_results = []
        for result in combined_results:
            # Normalize content/text fields first
            self._normalize_result_fields([result]) # Normalize in place
            
            result_id = result.get("id") or result.get("chunk_id")
            if result_id and result_id not in seen_ids:
                seen_ids.add(result_id)
                deduplicated_results.append(result)
        
        # Sort by score but return configured number of results for the LLM
        deduplicated_results.sort(key=lambda x: x.get("score", 0.0), reverse=True) # Use .get for safety
        # Return the configured number of results to allow LLM judgment
        final_results = deduplicated_results[:llm_results_limit] 
        
        # Add basic relevance metadata without filtering or scoring
        self._validate_search_results(query, query_info, final_results)
        
        # Record overall search metadata
        search_metadata["total_time"] = time.time() - search_start_time
        search_metadata["final_result_count"] = len(final_results)
        search_metadata["deduplicated_count"] = len(deduplicated_results)
        
        # Debug output if enabled
        if MEMNON_SETTINGS.get("debug", False):
            print("\n=== DEBUG: SEARCH RESULTS ===")
            print(f"Query: \"{query}\"")
            print(f"Query type: {query_info['type']}")
            print(f"Found {len(final_results)} results for LLM processing from {len(deduplicated_results)} deduplicated results")
            print("\n--- Vector search results ---")
            if "vector_search" in all_results:
                for i, result in enumerate(all_results["vector_search"]):
                    score = result.get("score", 0.0)
                    text = result.get("text", "")
                    if len(text) > 200:
                        text = text[:197] + "..."
                    print(f"Result {i+1} (Score: {score:.4f}): {text}")
            else:
                print("No vector search results found")
                
            print("\n--- Text search results ---")
            if "text_search" in all_results:
                for i, result in enumerate(all_results["text_search"]):
                    matched_keyword = result.get("metadata", {}).get("matched_keyword", "unknown")
                    text = result.get("text", "")
                    if len(text) > 200:
                        text = text[:197] + "..."
                    print(f"Result {i+1} (Matched: '{matched_keyword}'): {text}")
            else:
                print("No text search results found")
            
            print("\n--- Final results sent to LLM ---")
            for i, result in enumerate(final_results):
                source = result.get("source", "unknown")
                score = result.get("score", 0.0)
                
                # Extract metadata for display
                metadata_str = ""
                if "metadata" in result:
                    metadata = result.get("metadata", {})
                    meta_items = []
                    if "season" in metadata and "episode" in metadata:
                        meta_items.append(f"S{metadata['season']}E{metadata['episode']}")
                    if "scene_number" in metadata:
                        meta_items.append(f"Scene {metadata['scene_number']}")
                    if "matched_keyword" in metadata:
                        meta_items.append(f"Matched '{metadata['matched_keyword']}'")
                    
                    if meta_items:
                        metadata_str = f" [{', '.join(meta_items)}]"
                
                # Get text with truncation
                text = result.get("text", "")
                if len(text) > 200:
                    text = text[:197] + "..."
                
                print(f"Result {i+1} (Source: {source}, Score: {score:.4f}){metadata_str}: {text}")
            print("=============================\n")
            
        # Format final response
        response = {
            "query": query,
            "query_type": query_info["type"],
            "results": final_results, # Use the configured number of results for LLM
            "metadata": {
                "search_plan": search_plan["explanation"],
                "search_stats": search_metadata,
                "result_count": len(final_results), # Report the actual number returned
                "filters_applied": filters
            }
        }
        
        return response
    
    def _query_vector_search(self, query_text: str, collections: List[str], filters: Dict[str, Any], top_k: int) -> List[Dict[str, Any]]:
        """
        Query the vector database for chunks similar to the query text.
        
        Args:
            query_text: The text to search for
            collections: Vector collection names to search in
            filters: Metadata filters to apply (season, episode, etc.)
            top_k: Maximum number of results to return
            
        Returns:
            List of matching chunks with scores and metadata
        """
        if top_k is None:
            top_k = self.retrieval_settings["default_top_k"]
        
        try:
            # Generate embeddings for each model
            embeddings = {}
            for model_key in self.embedding_models:
                try:
                    embeddings[model_key] = self.generate_embedding(query_text, model_key)
                except Exception as e:
                    logger.error(f"Error generating {model_key} embedding: {e}")
            
            if not embeddings:
                logger.error("No embeddings generated for query")
                return []
            
            results = {}
            
            # Use direct raw SQL to avoid SQLAlchemy complexities with pgvector
            # Process each model separately
            for model_key, embedding in embeddings.items():
                # Connect directly using psycopg2
                import psycopg2
                from urllib.parse import urlparse
                
                # Parse database URL
                parsed_url = urlparse(self.db_url)
                username = parsed_url.username
                password = parsed_url.password
                database = parsed_url.path[1:]  # Remove leading slash
                hostname = parsed_url.hostname
                port = parsed_url.port or 5432
                
                # Connect to the database
                conn = psycopg2.connect(
                    host=hostname,
                    port=port,
                    user=username,
                    password=password,
                    database=database
                )
                
                try:
                    with conn.cursor() as cursor:
                        # Build filter conditions
                        filter_conditions = []
                        if filters:
                            if 'season' in filters:
                                filter_conditions.append(f"cm.season = {filters['season']}")
                            if 'episode' in filters:
                                filter_conditions.append(f"cm.episode = {filters['episode']}")
                            if 'world_layer' in filters:
                                filter_conditions.append(f"cm.world_layer = '{filters['world_layer']}'")
                        
                        filter_sql = " AND ".join(filter_conditions)
                        if filter_sql:
                            filter_sql = " AND " + filter_sql
                        
                        # Build embedding array as a string - pgvector expects [x,y,z] format
                        embedding_str = '[' + ','.join(str(x) for x in embedding) + ']'
                        
                        # Execute raw SQL query with pgvector's <=> operator
                        sql = f"""
                        SELECT 
                            nc.id, 
                            nc.raw_text, 
                            cm.season, 
                            cm.episode, 
                            cm.scene as scene_number,
                            1 - (ce.embedding <=> %s::vector) as score
                        FROM 
                            narrative_chunks nc
                        JOIN 
                            chunk_embeddings ce ON nc.id = ce.chunk_id
                        JOIN 
                            chunk_metadata cm ON nc.id = cm.chunk_id
                        WHERE 
                            ce.model = %s
                            {filter_sql}
                        ORDER BY 
                            score DESC
                        LIMIT 
                            %s
                        """
                        
                        # Execute the query
                        cursor.execute(sql, (embedding_str, model_key, top_k))
                        query_results = cursor.fetchall()
                        
                        # Process results
                        for result in query_results:
                            chunk_id, raw_text, season, episode, scene_number, score = result
                            chunk_id = str(chunk_id)
                            
                            if chunk_id not in results:
                                results[chunk_id] = {
                                    'id': chunk_id,
                                    'chunk_id': chunk_id,
                                    'text': raw_text,
                                    'content_type': 'narrative',
                                    'metadata': {
                                        'season': season,
                                        'episode': episode,
                                        'scene_number': scene_number
                                    },
                                    'model_scores': {},
                                    'score': 0.0,
                                    'source': 'vector_search'
                                }
                            
                            # Store score from this model
                            results[chunk_id]['model_scores'][model_key] = score
                finally:
                    conn.close()
            
            # Calculate weighted average scores
            model_weights = self.retrieval_settings['model_weights']
            for chunk_id, result in results.items():
                weighted_score = 0.0
                total_weight = 0.0
                
                for model_key, weight in model_weights.items():
                    if model_key in result['model_scores']:
                        weighted_score += result['model_scores'][model_key] * weight
                        total_weight += weight
                
                if total_weight > 0:
                    result['score'] = weighted_score / total_weight
            
            # Sort by final score and return top_k
            sorted_results = sorted(results.values(), key=lambda x: x['score'], reverse=True)
            return sorted_results[:top_k]
        
        except Exception as e:
            logger.error(f"Error in vector search: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def process_all_narrative_files(self, glob_pattern: str = None) -> int:
        """
        Process all narrative files matching the glob pattern.
        
        Args:
            glob_pattern: Pattern to match files to process. 
                          If None, uses the pattern from settings.json
            
        Returns:
            Total number of chunks processed
        """
        # Use pattern from settings if not provided
        if glob_pattern is None:
            glob_pattern = MEMNON_SETTINGS.get("import", {}).get("file_pattern", "ALEX_*.md")
        
        # Get batch size from settings
        batch_size = MEMNON_SETTINGS.get("import", {}).get("batch_size", 10)
        verbose = MEMNON_SETTINGS.get("import", {}).get("verbose", True)
        
        # Find all files matching the pattern
        import glob as glob_module
        files = glob_module.glob(glob_pattern)
        
        if not files:
            logger.warning(f"No files found matching pattern: {glob_pattern}")
            return 0
        
        # Log settings used
        logger.info(f"Processing files with pattern: {glob_pattern}")
        logger.info(f"Batch size: {batch_size}")
        
        total_chunks = 0
        for i, file_path in enumerate(files):
            logger.info(f"Processing file {i+1}/{len(files)}: {file_path}")
            try:
                chunks_processed = self.process_chunked_file(file_path)
                total_chunks += chunks_processed
                
                # Report progress
                if verbose:
                    self.interface.assistant_message(f"Processed {chunks_processed} chunks from {file_path}")
                
                # Process in batches to avoid overloading the system
                if batch_size > 0 and (i + 1) % batch_size == 0 and i < len(files) - 1:
                    logger.info(f"Completed batch of {batch_size} files. Taking a short break...")
                    time.sleep(2)  # Brief pause between batches
            
            except Exception as e:
                logger.error(f"Error processing file {file_path}: {e}")
                self.interface.assistant_message(f"Error processing {file_path}: {str(e)}")
        
        logger.info(f"Completed processing {total_chunks} total chunks from {len(files)} files")
        return total_chunks
    
    def step(self, messages: List[Message]) -> Any:
        """
        Process incoming messages and perform MEMNON functions.
        This is the main entry point required by Letta Agent framework.
        
        Args:
            messages: Incoming messages to process
            
        Returns:
            Agent response
        """
        # Extract the last user message
        if not messages:
            return "No messages to process."
        
        user_message = messages[-1]
        if user_message.role != "user":
            return "Expected a user message."
        
        # Extract text from message
        message_text = ""
        for content_item in user_message.content:
            if hasattr(content_item, "text") and content_item.text:
                message_text += content_item.text
        
        if not message_text.strip():
            return "I couldn't understand your message. Please provide a query or command."
        
        # Check for special commands first
        command = self._parse_command(user_message)
        
        # Handle special commands
        if command.get("action") == "process_files":
            # Process narrative files
            glob_pattern = command.get("pattern", "*_copy_notime.md")
            total_chunks = self.process_all_narrative_files(glob_pattern)
            return f"Processed {total_chunks} chunks from narrative files matching {glob_pattern}"
        
        elif command.get("action") == "status":
            # Return agent status
            return self._get_status()
            
        # Handle interactive mode with natural language queries
        elif "query" in command.get("action", "") or "search" in message_text.lower() or "?" in message_text:
            # Treat as natural language query
            logger.info(f"Processing natural language query: {message_text}")
            
            # Extract filters from command if present
            filters = command.get("filters", {})
            
            # Get default limit from settings
            default_limit = MEMNON_SETTINGS.get("query", {}).get("default_limit", 5)
            
            # Use unified query interface
            query_results = self.query_memory(
                query=message_text,
                query_type=None,  # Let the analysis determine the type
                filters=filters,
                k=default_limit
            )
            
            # Log search statistics
            strategies = query_results.get("metadata", {}).get("strategies_executed", [])
            result_count = query_results.get("metadata", {}).get("result_count", 0)
            logger.info(f"Query used strategies: {', '.join(strategies)} and found {result_count} results")
            
            # Generate synthesized response
            response = self._synthesize_response(
                query=message_text,
                results=query_results["results"],
                query_type=query_results["query_type"]
            )
            
            return response
            
        # Treat everything else as a natural language query
        else:
            logger.info(f"Treating message as natural language query: {message_text}")
            
            # Get default limit from settings
            default_limit = MEMNON_SETTINGS.get("query", {}).get("default_limit", 5)
            
            # Use unified query interface with LLM-directed search
            query_results = self.query_memory(
                query=message_text,
                query_type=None,  # Let the analysis determine the type
                filters={},
                k=default_limit
            )
            
            # Log search plan and statistics
            search_plan = query_results.get("metadata", {}).get("search_plan", "No search plan generated")
            logger.info(f"Search plan: {search_plan}")
            
            # Generate synthesized response
            response = self._synthesize_response(
                query=message_text,
                results=query_results["results"],
                query_type=query_results["query_type"]
            )
            
            # Add search metadata to response for debugging
            if "debug_info" not in response:
                response["debug_info"] = {}
            response["debug_info"]["search_plan"] = search_plan
            response["debug_info"]["query_type"] = query_results["query_type"]
            
            return response
    
    def _generate_search_plan(self, query: str, query_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a dynamic search plan for a given query using LLM reasoning.
        
        Args:
            query: The original query string
            query_info: Analyzed query information
            
        Returns:
            Search plan with strategies in priority order
        """
        import json
        import json
        
        # Prepare information about available data sources
        structured_tables = {
            "characters": "detailed information about characters including name, background, appearance, etc.",
            "places": "locations and settings with descriptions",
            "events": "significant narrative events",
            "character_relationships": "information about relationships between characters",
            "threats": "potential threats and dangers in the narrative",
            "factions": "groups and organizations"
        }
        
        vector_collections = {
            "narrative_chunks": "raw text passages from the narrative with embedded semantic meaning"
        }
        
        # Get the search plan prompt
        search_plan_prompt = MEMNON_SETTINGS.get("prompts", {}).get(
            "search_plan",
            "You are MEMNON, a sophisticated narrative intelligence system. Given a user query, your task is to create the optimal search strategy across multiple data sources."
        )
        
        # Construct the prompt without f-strings for the JSON template
        prompt = search_plan_prompt + """

QUERY: \"""" + query + """\"
QUERY TYPE: """ + query_info["type"] + """
ENTITIES MENTIONED: """ + json.dumps(query_info.get("entities", [])) + """
KEYWORDS: """ + json.dumps(query_info.get("keywords", [])) + """

AVAILABLE DATA SOURCES:
1. Structured tables (exact data lookup):
""" + json.dumps(structured_tables, indent=2) + """

2. Vector database (semantic search by meaning):
""" + json.dumps(vector_collections, indent=2) + """

3. Text search (direct keyword matching): 
Can perform fuzzy or exact matches on raw text.

Consider the following when creating your search plan:
- Which data source is most likely to contain the information needed?
- What order of operations would be most efficient?
- Could simpler searches be tried before more complex ones?
- How would you combine results from different sources?

YOUR RESPONSE MUST BE A SINGLE JSON OBJECT with this structure:

{
  "strategies": [
    {
      "type": "structured_data",
      "priority": 1, 
      "tables": ["characters", "places"]
    },
    {
      "type": "vector_search", 
      "priority": 2,
      "collections": ["narrative_chunks"]
    },
    {
      "type": "text_search",
      "priority": 3,
      "keywords": ["example", "keywords"]
    }
  ],
  "explanation": "Brief explanation of this search strategy (25 words max)"
}

IMPORTANT: Return only the JSON with no code block formatting, additional text, or commentary. Keep your explanation brief and do not repeat information.
"""
        
        # Query LLM for search plan
        llm_response = self._query_llm(prompt)  # Use default timeout from settings
        logger.info(f"LLM response for search plan: {llm_response[:500]}...")
        
        # Extract JSON from response
        import re
        import json
        logger.debug(f"Extracting JSON from LLM response of length {len(llm_response)}")
        
        # Try multiple patterns to extract JSON
        # First try to find JSON in code blocks (most reliable)
        json_match = re.search(r'```(?:json)?\s*(\{[\s\S]*?\})\s*```', llm_response)
        
        # Second, try to find JSON between header tags (common in some models)
        if not json_match:
            json_match = re.search(r'<json>\s*(\{[\s\S]*?\})\s*</json>', llm_response)
            
        # Third, look for the first complete JSON object with a strategies field (most specific)
        if not json_match:
            all_json_objects = re.finditer(r'\{[\s\S]*?\}', llm_response)
            for match in all_json_objects:
                try:
                    obj = json.loads(match.group(0))
                    if "strategies" in obj:
                        json_match = match
                        break
                except:
                    continue
        
        # Finally, fall back to any JSON-like structure
        if not json_match:
            json_match = re.search(r'(\{[\s\S]*?\})', llm_response)
        
        if not json_match:
            # Log the LLM response for debugging
            logger.warning(f"Could not extract JSON from LLM response. First 100 chars: {llm_response[:100]}...")
            
            # If response is too short or empty, create a default search plan
            if len(llm_response.strip()) < 20:
                logger.warning("LLM returned very short response, constructing default search plan")
                
                # Construct a basic search plan based on the query type
                search_plan = {
                    "strategies": [
                        {"type": "structured_data", "priority": 1, "tables": self._get_relevant_tables(query_info)},
                        {"type": "vector_search", "priority": 2, "collections": ["narrative_chunks"]},
                        {"type": "text_search", "priority": 3, "keywords": self._extract_search_keywords(query_info)}
                    ],
                    "explanation": f"Default search strategy for query type '{query_info['type']}'"
                }
                
                return search_plan
            else:
                raise ValueError(f"Could not extract JSON from LLM response. Try simplifying the search plan format.")
            
        json_str = json_match.group(1)
        
        # Add extra error handling for JSON parsing
        try:
            # Try to load the JSON directly
            search_plan = json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parsing error: {e}. Attempting to fix JSON before parsing...")
            
            try:
                # Sometimes the issue is with line breaks in the JSON - try removing them
                cleaned_json = json_str.replace('\n', ' ').replace('\r', '')
                search_plan = json.loads(cleaned_json)
                logger.info("Successfully parsed JSON after cleaning")
            except json.JSONDecodeError:
                try:
                    # If LLM outputs malformed JSON, we might need to extract the JSON more carefully
                    # Look for a complete, proper JSON object in the response
                    import re
                    better_json_match = re.search(r'(\{.*?"strategies".*?"explanation".*?\})', llm_response.replace('\n', ' '))
                    if better_json_match:
                        better_json = better_json_match.group(1)
                        search_plan = json.loads(better_json)
                        logger.info("Successfully parsed JSON with more careful extraction")
                    else:
                        # If we can't find a valid JSON, create a default one
                        logger.warning("Couldn't extract valid JSON. Creating default search plan.")
                        search_plan = {
                            "strategies": [
                                {"type": "structured_data", "priority": 1, "tables": self._get_relevant_tables(query_info)},
                                {"type": "vector_search", "priority": 2, "collections": ["narrative_chunks"]},
                                {"type": "text_search", "priority": 3, "keywords": self._extract_search_keywords(query_info)}
                            ],
                            "explanation": f"Default search strategy for '{query}'"
                        }
                except Exception as final_error:
                    logger.error(f"All JSON parsing attempts failed: {final_error}")
                    raise
        
        # Validate search plan structure
        if "strategies" not in search_plan or "explanation" not in search_plan:
            raise ValueError(f"Search plan missing required fields: {search_plan}")
        
        for strategy in search_plan["strategies"]:
            if "type" not in strategy or "priority" not in strategy:
                raise ValueError(f"Search strategy missing required fields: {strategy}")
            
            # Ensure each strategy has the right parameters
            if strategy["type"] == "structured_data" and "tables" not in strategy:
                strategy["tables"] = self._get_relevant_tables(query_info)
            elif strategy["type"] == "vector_search" and "collections" not in strategy:
                strategy["collections"] = ["narrative_chunks"]
            elif strategy["type"] == "text_search" and "keywords" not in strategy:
                strategy["keywords"] = self._extract_search_keywords(query_info)
        
        return search_plan
    
    def _parse_command(self, message: Message) -> Dict[str, Any]:
        """
        Parse a command from a user message.
        
        Args:
            message: User message
            
        Returns:
            Dictionary containing parsed command
        """
        # Extract text content from message
        if not message.content or len(message.content) == 0:
            return {}
        
        text = ""
        for content_item in message.content:
            if hasattr(content_item, "text") and content_item.text:
                text += content_item.text
        
        # Parse command (simple implementation)
        if "process" in text.lower() and "file" in text.lower():
            # Extract pattern if specified
            pattern = "*_copy_notime.md"  # Default
            pattern_match = re.search(r"pattern[:\s]+([^\s]+)", text)
            if pattern_match:
                pattern = pattern_match.group(1)
            
            return {"action": "process_files", "pattern": pattern}
        
        elif "query" in text.lower() or "search" in text.lower():
            # Extract query text
            query = text
            
            return {"action": "query", "query": query, "filters": {}}
            
        # Default: return empty dict if no command recognized
        return {}
    
    def _analyze_query(self, query: str, query_type: Optional[str]) -> Dict[str, Any]:
        """
        Analyze query to understand information need and type.
        Uses LLM to determine query type instead of algorithmic pattern matching.
        Removes entity extraction logic, relying on keywords and LLM context.
        
        Args:
            query: The query string
            query_type: Optional explicit query type
            
        Returns:
            Dict containing query analysis information
        """
        # Initialize query info with basic data
        query_info = {
            "query_text": query,
            "type": query_type if query_type else "general", # Will be overwritten by LLM if None
            "keywords": [],
            # Removed "entities" structure - rely on keywords/LLM context
            # Removed "time_references" - rely on keywords/LLM context or explicit filters
        }
        
        # If query_type is NOT provided, always use the LLM to determine it
        if not query_type:
            llm_query_type = self._determine_query_type_with_llm(query)
            if llm_query_type:
                query_info["type"] = llm_query_type
            else:
                # Default to general if LLM fails
                query_info["type"] = "general"
        
        # Extract simple keywords for search purposes (e.g., text search fallback)
        # Just use basic word splitting without complicated logic or stopword lists
        words = re.findall(r'\b\w+\b', query.lower())
        # Keep words longer than 2 chars, deduplicate
        query_info["keywords"] = sorted(list(set(word for word in words if len(word) > 2)))
        
        # Add the entire query as a special term if needed by search strategy?
        # Let's hold off on adding 'special_terms' unless a specific strategy needs it.
        # query_info["special_terms"] = [query.lower()]
        
        logger.debug(f"Analyzed query: Type='{query_info['type']}', Keywords={query_info['keywords']}")
        return query_info
    
    def _determine_query_type_with_llm(self, query: str) -> Optional[str]:
        """
        Use local LLM to determine the type of query.
        
        Args:
            query: The query string
            
        Returns:
            Detected query type or None if detection failed
        """
        # Get the prompt from the prompts section
        query_type_prompt = MEMNON_SETTINGS.get("prompts", {}).get(
            "analyze_query", 
            "Analyze the following narrative query and determine its primary type. Choose one of: character, location, event, theme, relationship, general."
        )
        
        prompt = f"""{query_type_prompt}

QUERY: "{query}"

Query types:
- 'character': Focused on a specific character, their actions, traits, development, etc.
- 'location': Focused on a place, setting, venue, etc.
- 'event': Focused on something that happened, a key moment, incident, etc.
- 'theme': Focused on ideas, motifs, recurring elements, or abstract concepts
- 'relationship': Focused on dynamics between two or more characters
- 'general': Does not fit clearly into any of the above categories

Output ONLY ONE WORD (the query type), with no explanation or additional text.
"""
        
        try:
            # Record start time
            start_time = time.time()
            
            # Call LM Studio API
            response = self._query_llm(prompt)
            
            # Calculate elapsed time
            elapsed = time.time() - start_time
            
            # Clean and extract only the first word from the response
            response = response.strip().lower()
            # Extract just the first word
            if " " in response:
                first_word = response.split()[0]
                logger.warning(f"LLM returned multi-word response: '{response}'. Extracting first word: '{first_word}'")
                response = first_word
                
            valid_types = ["character", "location", "event", "theme", "relationship", "general"]
            
            if response in valid_types:
                logger.info(f"LLM classified query as type: {response} (determined in {elapsed:.2f}s)")
                return response
            else:
                logger.warning(f"LLM returned invalid query type: '{response}'")
                logger.warning(f"Full response was: '{response}'")
                # Fall back to 'general' if the response is invalid
                logger.info(f"Falling back to 'general' query type")
                return "general"
                
        except Exception as e:
            logger.error(f"Error determining query type with LLM: {e}")
            return None
    
    def _query_llm(self, prompt: str, timeout: int = None, retry_with_chat: bool = True) -> str:
        """
        Query the local LLM with timeout protection and retry logic.
        
        Args:
            prompt: The prompt to send to the LLM
            timeout: Maximum time to wait for a response in seconds
            retry_with_chat: Whether to retry with chat API if completions API fails
            
        Returns:
            LLM response as a string or empty string on failure
        """
        # Use global settings for LLM parameters
        global_llm_settings = GLOBAL_SETTINGS.get("llm", {})
        
        # Get API endpoint from settings or use default
        api_base = global_llm_settings.get("api_base", "http://localhost:1234")
        completions_endpoint = f"{api_base}/v1/completions"
        chat_endpoint = f"{api_base}/v1/chat/completions"
        
        # Get parameters from global settings
        temperature = global_llm_settings.get("temperature", 0.8)
        top_p = global_llm_settings.get("top_p", 0.95)
        top_k = global_llm_settings.get("top_k", 40)
        min_p = global_llm_settings.get("min_p", 0.05)
        max_tokens = global_llm_settings.get("max_tokens", 2048)
        
        # Use provided timeout or get from settings
        if timeout is None:
            timeout = global_llm_settings.get("timeout", 30)
            
        repeat_penalty = global_llm_settings.get("repeat_penalty", 1.1)
        presence_penalty = global_llm_settings.get("presence_penalty", 0.5)
        frequency_penalty = global_llm_settings.get("frequency_penalty", 0.5)
        
        try:
            # Add a context reset for each new query
            reset_token = f"\n\n--- NEW QUERY (ID: {int(time.time())}) ---\n\n"
            reset_prompt = reset_token + prompt
            
            # Use LM Studio API completions endpoint first
            payload = {
                "model": self.model_id,
                "prompt": reset_prompt,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "min_p": min_p,
                "max_tokens": max_tokens,
                "repeat_penalty": repeat_penalty,
                "stream": False
            }
            
            logger.info(f"Sending request to LLM ({self.model_id}) for query analysis (timeout: {timeout}s)")
            logger.debug(f"API endpoint: {completions_endpoint}")
            headers = {"Content-Type": "application/json"}
            
            try:
                response = requests.post(completions_endpoint, json=payload, headers=headers, timeout=timeout)
                
                if response.status_code == 200:
                    response_data = response.json()
                    
                    if "choices" in response_data and len(response_data["choices"]) > 0:
                        if "text" in response_data["choices"][0]:
                            return response_data["choices"][0]["text"]
                    
                    # Try alternate response format
                    if "choices" in response_data and len(response_data["choices"]) > 0:
                        if "message" in response_data["choices"][0]:
                            return response_data["choices"][0]["message"]["content"]
                    
                    logger.warning(f"Unexpected LLM response format: {response_data}")
            except requests.Timeout:
                logger.warning(f"Completions endpoint request timed out after {timeout}s")
            except Exception as e:
                logger.warning(f"Error with completions endpoint: {e}")
            
            # Try fallback to chat completions if normal completions fail and retry_with_chat is enabled
            if retry_with_chat:
                logger.info(f"Trying chat completions endpoint as fallback (timeout: {timeout}s)")
                
                # Get system prompt from MEMNON prompts
                system_prompt = MEMNON_SETTINGS.get("prompts", {}).get("system", "You are a narrative analysis assistant.")
                
                # Add a reset token to the user message
                reset_token = f"\n\n--- NEW QUERY (ID: {int(time.time())}) ---\n\n"
                reset_prompt = reset_token + prompt
                
                chat_payload = {
                    "model": self.model_id,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": reset_prompt}
                    ],
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "min_p": min_p,
                    "max_tokens": max_tokens,
                    "repeat_penalty": repeat_penalty
                }
                
                logger.debug(f"Trying chat endpoint: {chat_endpoint}")
                try:
                    response = requests.post(chat_endpoint, json=chat_payload, headers=headers, timeout=timeout)
                    
                    if response.status_code == 200:
                        response_data = response.json()
                        
                        if "choices" in response_data and len(response_data["choices"]) > 0:
                            if "message" in response_data["choices"][0]:
                                return response_data["choices"][0]["message"]["content"]
                    
                    logger.warning(f"Chat completions endpoint failed: {response.status_code}")
                except requests.Timeout:
                    logger.warning(f"Chat completions endpoint request timed out after {timeout}s")
                except Exception as e:
                    logger.warning(f"Error with chat completions endpoint: {e}")
        
        except Exception as e:
            logger.error(f"Error in LLM query: {e}")
            logger.error(f"Error details: {str(e)}")
        
        # Return a default response if all attempts fail
        logger.warning("All LLM query attempts failed, returning empty response")
        return ""
    
    def _determine_relevant_memory_tiers(self, query_info: Dict[str, Any]) -> List[str]:
        """
        Determine which memory tiers are most relevant to a query.
        
        Args:
            query_info: Analyzed query information
            
        Returns:
            List of memory tier names in priority order
        """
        query_type = query_info.get("type", "general")
        
        # Get memory tier mapping from query types
        if query_type in self.query_types:
            tier_info = self.query_types[query_type]
            primary_tier = tier_info.get("primary_tier")
            secondary_tier = tier_info.get("secondary_tier")
            
            tiers = []
            if primary_tier:
                tiers.append(primary_tier)
            if secondary_tier:
                tiers.append(secondary_tier)
                
            # Always include narrative tier if not already included
            if "narrative" not in tiers:
                tiers.append("narrative")
                
            return tiers
        
        # Default to all memory tiers
        return list(self.memory_tiers.keys())
    
    def _get_relevant_tables(self, query_info: Dict[str, Any]) -> List[str]:
        """
        Determine which database tables are most relevant to this query.
        Simplified to focus on type, letting LLM plan guide specifics.
        
        Args:
            query_info: Analyzed query information
            
        Returns:
            List of relevant table names
        """
        query_type = query_info["type"]
        # Basic mapping, LLM plan can override/refine this
        if query_type == "character":
            return ["characters"]
        elif query_type == "location":
            return ["places"]
        elif query_type == "event":
            return ["events"] # Assuming an 'events' table exists or is planned
        elif query_type == "relationship":
             # Relationships might involve characters table primarily
             return ["characters", "character_relationships"] # Assuming relationship table
        else:  # general or unknown - let LLM plan decide or use broad defaults
            return ["characters", "places"] # Default broad tables
        
    def _get_relevant_collections(self, query_info: Dict[str, Any]) -> List[str]:
        """
        Determine which vector collections are most relevant to this query.
        
        Args:
            query_info: Analyzed query information
            
        Returns:
            List of relevant collection names
        """
        # Currently we only have one collection, but this will be useful
        # when more specialized collections are added
        return ["narrative_chunks"]
    
    def _extract_search_keywords(self, query_info: Dict[str, Any]) -> List[str]:
        """
        Extract keywords for text search from query info.
        Uses the simplified keywords from _analyze_query.
        
        Args:
            query_info: Analyzed query information
            
        Returns:
            List of keywords for text search
        """
        keywords = query_info.get("keywords", [])
        
        # If no keywords were extracted, maybe use the full query text?
        if not keywords and query_info.get("query_text"):
             # Split the query text itself as a fallback
             words = re.findall(r'\b\w+\b', query_info["query_text"].lower())
             keywords = sorted(list(set(word for word in words if len(word) > 2)))

        # Ensure the original query is considered if keyword list is short or empty
        if not keywords or len(keywords) < 3:
             raw_query = query_info.get("query_text")
             if raw_query and raw_query not in keywords:
                 keywords.append(raw_query) # Add the full query string

        # Remove duplicates that might have been introduced
        return sorted(list(set(keywords)))
    
    def _query_structured_data(self, query_info: Dict[str, Any], tables: List[str], 
                             filters: Optional[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        """
        Query structured database tables for exact data lookup.
        Removes hardcoded scoring, assigns a neutral score or relies on natural DB order.
        
        Args:
            query_info: Query analysis information
            tables: List of tables to query
            filters: Optional filters to apply
            k: Number of results to return
            
        Returns:
            List of database query results
        """
        session = self.Session()
        results = []
        
        try:
            # Extract potential entities/keywords from the query for filtering
            search_terms = query_info.get("keywords", [])
            # Include the raw query text split into words for broader matching
            search_terms.extend(re.findall(r'\b\w+\b', query_info.get("query_text", "").lower()))
            search_terms = list(set(term for term in search_terms if len(term) > 2)) # Deduplicate and filter length

            # Process each table
            for table_name in tables:
                table_results = []
                if table_name == "characters":
                    char_query = session.query(Character)
                    
                    # Build filter conditions based on search terms matching name or aliases
                    conditions = []
                    if search_terms:
                        for term in search_terms:
                             conditions.append(Character.name.ilike(f"%{term}%"))
                             conditions.append(Character.aliases.any(term)) # Check array

                    if conditions:
                         char_query = char_query.filter(sa.or_(*conditions))

                    # Apply explicit filters if provided
                    if filters:
                         if "character_name" in filters:
                            char_query = char_query.filter(Character.name == filters["character_name"])
                         # Add other potential character filters here

                    # Get results
                    char_results = char_query.limit(k).all()
                    
                    # Transform to dict format - assign neutral score
                    for char in char_results:
                        table_results.append({
                            "id": str(char.id),
                            "chunk_id": f"char_{char.id}", # Create a unique ID for deduplication
                            "type": "character",
                            "content": { # Keep structured content
                                "name": char.name,
                                "aliases": char.aliases,
                                "summary": char.summary,
                                "appearance": char.appearance,
                                "personality": char.personality,
                                "background": char.background,
                                "current_location": char.current_location
                            },
                            "text": f"Character: {char.name}. Summary: {char.summary}", # Provide text version for LLM
                            "score": 0.8, # Assign a default score, slightly lower than text match
                            "metadata": {
                                "source": "structured_data",
                                "table": "characters"
                            }
                        })
                
                elif table_name == "places":
                    place_query = session.query(Place)
                    
                    # Build filter conditions based on search terms matching name or location
                    conditions = []
                    if search_terms:
                         for term in search_terms:
                             conditions.append(Place.name.ilike(f"%{term}%"))
                             conditions.append(Place.location.ilike(f"%{term}%")) # Check location field too
                             conditions.append(Place.type.ilike(f"%{term}%")) # Check type field

                    if conditions:
                         place_query = place_query.filter(sa.or_(*conditions))

                    # Apply explicit filters
                    if filters:
                        if "place_name" in filters:
                            place_query = place_query.filter(Place.name == filters["place_name"])
                        # Add other potential place filters here

                    # Get results
                    place_results = place_query.limit(k).all()
                    
                    # Transform to dict format - assign neutral score
                    for place in place_results:
                        table_results.append({
                            "id": str(place.id),
                             "chunk_id": f"place_{place.id}", # Create a unique ID for deduplication
                            "type": "place",
                            "content": { # Keep structured content
                                "name": place.name,
                                "type": place.type,
                                "location": place.location,
                                "summary": place.summary,
                                "inhabitants": place.inhabitants,
                                "historical_significance": place.historical_significance
                            },
                            "text": f"Place: {place.name} ({place.type}). Location: {place.location}. Summary: {place.summary}", # Text version
                            "score": 0.8, # Assign a default score
                            "metadata": {
                                "source": "structured_data",
                                "table": "places"
                            }
                        })
                
                # Add additional table handlers here as needed (e.g., events, relationships)
                
                # Add results from this table
                results.extend(table_results)

            # No sorting needed here, let the final sort handle it
            return results[:k] # Apply limit at the end of processing all tables for this strategy
            
        except Exception as e:
            logger.error(f"Error querying structured data: {e}")
            return []
            
        finally:
            session.close()
    
    def _query_text_search(self, query_info: Dict[str, Any], keywords: List[str], 
                         filters: Optional[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        """
        Perform direct text search using SQL LIKE.
        Removes score calculation based on occurrences, assigns a simple score for match.
        
        Args:
            query_info: Query analysis information
            keywords: Terms to search for
            filters: Optional filters to apply
            k: Number of results to return
            
        Returns:
            List of text search results
        """
        session = self.Session()
        results = {} # Use dict for deduplication during collection
        
        if not keywords:
             logger.warning("Text search called with no keywords.")
             return []

        try:
            # Build SQL query using text expressions
            for keyword in keywords:
                # Skip very short keywords unless it's the only one
                if len(keyword) < 3 and len(keywords) > 1:
                    continue
                    
                # Create pattern with SQL LIKE (case-insensitive)
                pattern = f"%{keyword}%"
                
                # Query for chunks containing this keyword
                query = session.query(
                    NarrativeChunk.id,
                    NarrativeChunk.raw_text,
                    ChunkMetadata.season,
                    ChunkMetadata.episode,
                    ChunkMetadata.scene.label('scene_number')
                ).join(
                    ChunkMetadata, NarrativeChunk.id == ChunkMetadata.chunk_id, isouter=True # Use outer join if metadata might be missing
                ).filter(
                    NarrativeChunk.raw_text.ilike(pattern)
                )
                
                # Apply filters if provided
                if filters:
                    if 'season' in filters and filters['season'] is not None:
                        query = query.filter(ChunkMetadata.season == filters['season'])
                    if 'episode' in filters and filters['episode'] is not None:
                        query = query.filter(ChunkMetadata.episode == filters['episode'])
                    if 'world_layer' in filters and filters['world_layer']:
                        query = query.filter(ChunkMetadata.world_layer == filters['world_layer'])
                
                # Limit results per keyword to avoid bias towards common terms, then combine
                chunks = query.limit(k).all() # Limit per keyword search
                
                for chunk in chunks:
                    chunk_id = str(chunk.id)
                    
                    # Only add if not already found (or update if new keyword matches?)
                    # For simplicity, just keep the first match found for an ID
                    if chunk_id not in results:
                        results[chunk_id] = {
                            "id": chunk_id,
                            "chunk_id": chunk_id, # Keep for consistency
                            "type": "narrative_chunk",
                            "text": chunk.raw_text, # Use 'text' field consistently
                            "score": 0.9, # Assign a fixed score for text match presence
                            "metadata": {
                                "season": chunk.season,
                                "episode": chunk.episode,
                                "scene_number": chunk.scene_number,
                                "source": "text_search",
                                "matched_keyword": keyword # Keep track of which keyword matched first
                            }
                        }

            # Convert dict back to list
            final_results = list(results.values())

            # No sorting needed here, final sort in query_memory handles it
            return final_results[:k] # Apply overall limit after combining keywords
            
        except Exception as e:
            logger.error(f"Error in text search: {e}")
            return []
            
        finally:
            session.close()
    
    def _normalize_result_fields(self, results: List[Dict[str, Any]]) -> None:
        """
        Ensure all results have consistent field names by copying 'content' to 'text'
        when needed.
        
        Args:
            results: List of search results to normalize
            
        Returns:
            None (modifies results in place)
        """
        for result in results:
            # If result has 'content' but no 'text', copy content to text
            if "content" in result and "text" not in result:
                # Handle different content types
                if isinstance(result["content"], str):
                    result["text"] = result["content"]
                else:
                    # For structured data, convert content dict to string
                    result["text"] = str(result["content"])
                
                logger.debug(f"Normalized result: copied 'content' to 'text' field for result {result.get('id', 'unknown')}")
                
    def _validate_search_results(self, query: str, query_info: Dict[str, Any], results: List[Dict[str, Any]]) -> None:
        """
        Simply record any relevant terms found in the results without judging their relevance.
        No filtering or scoring adjustments - let the LLM do all relevance assessment.
        Modified to match the guideline in memnon_refactor.md.
        
        Args:
            query: Original query string
            query_info: Analyzed query information
            results: Results to validate (modified in place)
        """
        # Get query terms for metadata context
        query_terms = query_info.get("keywords", [])
        if not query_terms and query_info.get("query_text"):
             # Use raw query split if keywords are missing
             query_terms = re.findall(r'\b\w+\b', query_info["query_text"].lower())
             query_terms = [term for term in query_terms if len(term) > 2]

        # Process each result to add minimal metadata about query terms
        for result in results:
            # No relevance assessment - just record query info for LLM reference
            result["relevance"] = {
                "query": query.lower(),
                "query_terms": query_terms # Store the keywords used/extracted
                # Removed score adjustments
                # Removed has_relevant_terms, matches_special_terms flags
            }
            # Ensure score exists, default to 0 if missing before sorting/LLM step
            if "score" not in result:
                 result["score"] = 0.0

            # Optional: Check if text/content exists, add placeholder if missing
            if "text" not in result and "content" not in result:
                 result["text"] = "[Content Missing]"
    
    def _synthesize_response(self, query: str, results: List[Dict[str, Any]], query_type: str) -> str:
        """
        Generate a synthesized narrative response using LLM.
        
        Args:
            query: The original query
            results: Synthesized results
            query_type: Type of query
            
        Returns:
            Synthesized response as a string
        """
        # Even with no results, we'll let the LLM handle the response
        if not results:
            # Add minimal placeholder context rather than short-circuiting
            results = [{"text": "[No exact matches found in memory]", "source": "placeholder"}]
        
        # Get LLM results limit from settings
        llm_results_limit = MEMNON_SETTINGS.get("query", {}).get("llm_results_limit", 20)
        
        # Get the response synthesis prompt
        synthesis_prompt = MEMNON_SETTINGS.get("prompts", {}).get(
            "response_synthesis",
            "You are MEMNON, an advanced narrative intelligence system with perfect recall of story elements. Answer the following query based only on the provided context and evidence."
        )
        
        # Build LLM prompt
        prompt = f"""{synthesis_prompt}

QUERY: "{query}"
QUERY TYPE: {query_type}

RELEVANT CONTEXT:
"""
        
        # Add results as context up to the configured limit
        for i, result in enumerate(results[:llm_results_limit]):
            source_type = result.get("source", "unknown")
            prompt += f"\n--- Source {i+1} ({source_type}) ---\n"
            
            # Check for text field (main content)
            if "text" in result:
                prompt += f"TEXT: {result['text']}\n"
            
            # Add metadata if available
            if "metadata" in result:
                meta_str = ", ".join([f"{k}: {v}" for k, v in result["metadata"].items() 
                                    if k in ["season", "episode", "scene_number"]])
                if meta_str:
                    prompt += f"METADATA: {meta_str}\n"
            
            # No special handling for query types - let the LLM figure it out
        
        prompt += "\n\nBased only on the information above, provide a concise answer to the query. Start your answer with 'ANSWER:' followed by a summary of ALL relevant information, no matter how fragmented or incomplete it may be. Use EVERY piece of context that could relate to the query, even tangential connections. Never respond with 'I don't know' or 'I don't have information' - instead, summarize whatever information you can find, noting how confident you are and what details are available. Focus on being thorough but concise, about 3-5 sentences. DO NOT repeat yourself or restate the same information - avoid loops and redundancy."
        
        try:
            # Query LLM for synthesized response
            response = self._query_llm(prompt)
            
            # Clean up response
            response = response.strip()
            
            # If response is empty or couldn't be generated, create a simple summary
            if not response:
                response = self._generate_basic_summary(query, results, query_type)
            
            return response
        
        except Exception as e:
            logger.error(f"Error synthesizing response: {e}")
            return self._generate_basic_summary(query, results, query_type)
    
    def _generate_basic_summary(self, query: str, results: List[Dict[str, Any]], query_type: str) -> str:
        """
        Generate a basic summary if LLM synthesis fails.
        
        Args:
            query: The original query
            results: Results from search
            query_type: Type of query
            
        Returns:
            Basic summary as a string
        """
        summary = f"Here's what I found about your query:\n\n"
        
        for i, result in enumerate(results[:3]):  # Include top 3 results
            source_type = result.get("source", "unknown")
            score = result.get("score", 0)
            summary += f"- Result {i+1} ({source_type}, Score: {score:.2f}):\n"
            
            # Add text content
            if "text" in result:
                # Truncate text if needed
                content = result["text"]
                if len(content) > 200:
                    content = content[:197] + "..."
                
                # Add metadata if available
                if "metadata" in result and "season" in result["metadata"] and "episode" in result["metadata"]:
                    summary += f"  Scene (S{result['metadata']['season']}E{result['metadata']['episode']}): {content}\n"
                else:
                    summary += f"  {content}\n"
            
            else:
                summary += f"- {result['type']}: {str(result['content'])[:100]}...\n"
        
        return summary
    
    def _format_query_results(self, results: List[Dict[str, Any]]) -> str:
        """
        Format query results for display.
        
        Args:
            results: List of result dictionaries
            
        Returns:
            Formatted results as a string
        """
        if not results:
            return "No results found."
        
        output = f"Found {len(results)} results:\n\n"
        
        for i, result in enumerate(results):
            output += f"Result {i+1} (Score: {result['score']:.4f}):\n"
            output += f"Chunk ID: {result['chunk_id']}\n"
            output += f"Season: {result['metadata']['season']}, Episode: {result['metadata']['episode']}\n"
            output += f"Scene: {result['metadata']['scene_number']}\n"
            
            # Truncate text if too long
            text = result['text']
            if len(text) > 500:
                text = text[:497] + "..."
            output += f"Content: {text}\n\n"
        
        return output
    
    def _get_status(self) -> str:
        """
        Get the current status of the MEMNON agent.
        
        Returns:
            Status information as a string
        """
        session = self.Session()
        try:
            # Get counts from database
            chunk_count = session.query(sa.func.count(NarrativeChunk.id)).scalar()
            embedding_count = session.query(sa.func.count(ChunkEmbedding.chunk_id)).scalar()
            
            # Count by model
            model_counts = {}
            for model_key in self.embedding_models.keys():
                count = session.query(sa.func.count(ChunkEmbedding.chunk_id)) \
                    .filter(ChunkEmbedding.model == model_key) \
                    .scalar()
                model_counts[model_key] = count
            
            # Get embedding models info
            embedding_models_info = [f"{model}: {type(model_obj).__name__}" 
                                   for model, model_obj in self.embedding_models.items()]
            
            status = f"MEMNON Status:\n"
            status += f"- Database: {self.db_url}\n"
            status += f"- Total chunks: {chunk_count}\n"
            status += f"- Total embeddings: {embedding_count}\n"
            status += f"- Embeddings by model: {json.dumps(model_counts, indent=2)}\n"
            status += f"- Loaded embedding models: {', '.join(embedding_models_info)}\n"
            
            return status
        
        except Exception as e:
            logger.error(f"Error getting status: {e}")
            return f"Error getting status: {e}"
        
        finally:
            session.close()